<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Eugene Zhulenev]]></title>
  <link href="http://eugenezhulenev.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://eugenezhulenev.com/"/>
  <updated>2019-01-01T15:36:44-08:00</updated>
  <id>http://eugenezhulenev.com/</id>
  <author>
    <name><![CDATA[Eugene Zhulenev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Optimizing Spark Machine Learning for Small Data]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/09/16/spark-ml-for-big-and-small-data/"/>
    <updated>2015-09-16T07:04:25-07:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/09/16/spark-ml-for-big-and-small-data</id>
    <content type="html"><![CDATA[<blockquote>
  <p><strong>Update 2015-10-08</strong>: Optimization “hack” described in this post still works, however we don’t use it in production anymore. 
With careful parallelism config, overhead introduced by distributed models is negligible.</p>
</blockquote>

<p>You’ve all probably already know how awesome is Spark for doing Machine Learning on Big Data. However I’m pretty sure
no one told you how bad (slow) it can be on Small Data. </p>

<p>As I mentioned in my <a href="/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines">previous post</a>, we
extensively use Spark for doing machine learning and audience modeling. It turned out that in some cases, for example when
we are starting optimization for new client/campaign we simply don’t have enough positive examples to construct big enough dataset, so that
using Spark would make sense.</p>

<!-- more -->

<h3 id="spark-ml-from-10000-feet">Spark ML from 10000 feet</h3>

<p>Essentially every machine learning algorithm is a function minimization, where function value depends on some calculation using data in <code>RDD</code>.
For example logistic regression can calculate function value 1000 times before it will converge and find optimal parameters. It means that it will 
compute some <code>RDD</code> 1000 times. In case of <code>LogisticRegression</code> it’s doing <code>RDD.treeAggregate</code> which is supper efficient, but still it’s distributed 
computation.</p>

<p>Now imagine that all the data you have is 50000 rows, and you have for example 1000 partitions. It means that each partition has only 50 rows. And 
each <code>RDD.treeAggregate</code> on every iteration serializing closures, sending them to partitions and collecting result back. 
It’s <strong>HUGE OVERHEAD</strong> and huge load on a driver.</p>

<h3 id="throw-away-spark-and-use-pythonr">Throw Away Spark and use Python/R?</h3>

<p>It’s definitely an option, but we don’t want to build multiple systems for data of different size. Spark ML pipelines are awesome abstraction,
and we want to use it for all machine learning jobs. Also we want to use the same algorithm, so results would be consistent if dataset size
just crossed the boundary between small and big data.</p>

<h3 id="run-logisticregression-in-local-mode">Run LogisticRegression in ‘Local Mode’</h3>

<p>What if Spark could run the same machine learning algorithm, but instead of using <code>RDD</code> for storing input data, it would use <code>Arrays</code>?
It solves all the problems, you get consistent model, computed 10-20x faster because it doesn’t need distributed computations.</p>

<p>That’s exactly approach I used in <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a>, it’s called <a href="https://github.com/collectivemedia/spark-ext/blob/b61d73bbf6ce38c6b9fe37764934f37b640081fb/sparkext-mllib/src/main/scala/org/apache/spark/ml/classification/LocalLogisticRegression.scala">LocalLogisticRegression</a>.
It’s mostly copy-pasta from Spark <code>LogisticRegression</code>, but when input data frame has only single partition, it’s running
function optimization on one of the executors using <code>mapPartition</code> function, essentially using Spark as distributed executor service.</p>

<p>This approach is much better than collecting data to driver, because you are not limited by driver computational resources.</p>

<p>When <code>DataFrame</code> has more than 1 partition it just falls back to default distributed logistic regression.</p>

<p>Code for new <code>train</code> method looks like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">def</span> <span class="function">trainLocal</span>(
      <span class="key">instances</span>: <span class="predefined-type">Array</span>[(<span class="predefined-type">Double</span>, <span class="predefined-type">Vector</span>)]
    ): (LogisticRegressionModel, <span class="predefined-type">Array</span>[<span class="predefined-type">Double</span>]) = <span class="error">…</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">train</span>(<span class="key">dataset</span>: DataFrame): LogisticRegressionModel = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">if</span> (dataset.rdd.partitions.length == <span class="integer">1</span>) {
    log.info(s<span class="error">”</span>Build LogisticRegression <span class="keyword">in</span> local mode<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val (model, objectiveHistory) = extractLabeledPoints(dataset).map {
  <span class="keyword">case</span> LabeledPoint(<span class="key">label</span>: <span class="predefined-type">Double</span>, <span class="key">features</span>: <span class="predefined-type">Vector</span>) =&amp;gt; (label, features)
}.mapPartitions { instances =&amp;gt;
  Seq(trainLocal(instances.toArray)).toIterator
}.first()

val logRegSummary = <span class="keyword">new</span> BinaryLogisticRegressionTrainingSummary(
  model.transform(dataset),
  probabilityCol,
  labelCol,
  objectiveHistory)
model.setSummary(logRegSummary)
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;} <span class="keyword">else</span> {
    log.info(s<span class="error">”</span>Fallback to distributed LogisticRegression<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val that = classOf[LogisticRegression].getConstructor(classOf[<span class="predefined-type">String</span>]).newInstance(uid)
val logisticRegression = copyValues(that)
<span class="comment">// Scala Reflection magic to call protected train method</span>
...
logisticRegression.train(dataset)   } }       </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<p>If input dataset size is less than 100000 rows, it will be placed inside single partition, and regression model will be trained in local mode.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val <span class="key">base</span>: DataFrame = <span class="error">…</span>
val datasetPartitionSize = <span class="integer">100000</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Compute optimal partitions size based on base join</span>
val baseSize = base.count()
val numPartitions = (baseSize.toDouble / datasetPartitionSize).ceil.toInt
log.debug(s<span class="error">”</span>Response base <span class="key">size</span>: <span class="error">$</span>baseSize<span class="error">”</span>)
log.debug(s<span class="error">”</span>Repartition dataset using <span class="error">$</span>numPartitions partitions<span class="error">”</span>)
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="results">Results</h2>

<p>With a little ingenuity (and copy paste) Spark became perfect tool for machine learning both on Small and Big Data. Most awesome thing is that this
new <code>LocalLogisticRegression</code> can be used as drop in replacement in Spark ML pipelines, producing exactly the same <code>LogisticRegressionModel</code> at the end.</p>

<p>It might be interesting idea to use this approach in Spark itself, because in this case it would be possible to do it
without doing so many code duplication. I’d love to see if anyone else had the same problem, and how solved it.</p>

<blockquote>
  <p>More cool Spark things in <a href="https://github.com/collectivemedia/spark-ext/">Github</a>.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Audience Modeling With Spark ML Pipelines]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines/"/>
    <updated>2015-09-09T06:04:25-07:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines</id>
    <content type="html"><![CDATA[<p>At <a href="http://collective.com">Collective</a> we are heavily relying on machine learning and predictive modeling to 
run digital advertising business. All decisions about what ad to show at this particular time to this particular user
are made by machine learning models (some of them are real time, and some of them are offline).</p>

<p>We have a lot of projects that uses machine learning, common name for all of them can be <strong>Audience Modeling</strong>, as they
all are trying to predict audience conversion (<em>CTR, Viewability Rate, etc…</em>) based on browsing history, behavioral segments and other type of 
predictors.</p>

<p>For most of new development we use <a href="https://spark.apache.org">Spark</a> and <a href="https://spark.apache.org/mllib/">Spark MLLib</a>. It is a awesome project,
however we found that some nice tools/libraries that are widely used for example in R are missing in Spark. In order to add missing
features that we would really like to have in Spark, we created <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> - Spark Extensions
Library. </p>

<blockquote>
  <p>Spark Ext on Github: <a href="https://github.com/collectivemedia/spark-ext">https://github.com/collectivemedia/spark-ext</a></p>
</blockquote>

<p>I’m going to show simple example of combining <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> with Spark ML pipelines for predicting user conversions based geo and browsing history data.</p>

<blockquote>
  <p>Spark ML pipeline example: <a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala">SparkMlExtExample.scala</a></p>
</blockquote>

<!-- more -->

<h2 id="predictors-data">Predictors Data</h2>

<p>I’m using dataset with 2 classes, that will be used for solving classification problem (user converted or not). It’s created with 
<a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/DataGenerator.scala">dummy data generator</a>, 
so that these 2 classes can be easily separated. It’s pretty similar to real data that usually available in digital advertising.</p>

<h3 id="browsing-history-log">Browsing History Log</h3>

<p>History of web sites that were visited by user.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie          | Site          | Impressions&lt;br /&gt;
————— |————– | ————-
wKgQaV0lHZanDrp | live.com      | 24
wKgQaV0lHZanDrp | pinterest.com | 21
rfTZLbQDwbu5mXV | wikipedia.org | 14
rfTZLbQDwbu5mXV | live.com      | 1
rfTZLbQDwbu5mXV | amazon.com    | 1
r1CSY234HTYdvE3 | youtube.com   | 10
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="geo-location-log">Geo Location Log</h3>

<p>Latitude/Longitude impression history.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie          | Lat     | Lng       | Impressions
————— |———| ——— | ————
wKgQaV0lHZanDrp | 34.8454 | 77.009742 | 13
wKgQaV0lHZanDrp | 31.8657 | 114.66142 | 1
rfTZLbQDwbu5mXV | 41.1428 | 74.039600 | 20
rfTZLbQDwbu5mXV | 36.6151 | 119.22396 | 4
r1CSY234HTYdvE3 | 42.6732 | 73.454185 | 4
r1CSY234HTYdvE3 | 35.6317 | 120.55839 | 5
20ep6ddsVckCmFy | 42.3448 | 70.730607 | 21
20ep6ddsVckCmFy | 29.8979 | 117.51683 | 1
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="transforming-predictors-data">Transforming Predictors Data</h2>

<p>As you can see predictors data (sites and geo) is in <em>long</em> format, each <code>cookie</code> has multiple rows associated with it,
and it’s in general is not a good fit for machine learning.
We’d like <code>cookie</code> to be a primary key, and all other data should form <code>feature vector</code>.</p>

<h3 id="gather-transformer">Gather Transformer</h3>

<p>Inspired by R <code>tidyr</code> and <code>reshape2</code> packages. Convert <em>long</em> <code>DataFrame</code> with values
for each key into <em>wide</em> <code>DataFrame</code>, applying aggregation function if single
key has multiple values.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val gather = <span class="keyword">new</span> Gather()
      .setPrimaryKeyCols(<span class="error">“</span>cookie<span class="error">”</span>)
      .setKeyCol(<span class="error">“</span>site<span class="error">”</span>)
      .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
      .setValueAgg(<span class="error">“</span>sum<span class="error">”</span>)         <span class="comment">// sum impression by key</span>
      .setOutputCol(<span class="error">“</span>sites<span class="error">”</span>)
val gatheredSites = gather.transform(siteLog)    &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | Sites
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { site: live.com, impressions: 24.0 }, 
                 |  { site: pinterest.com, impressions: 21.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { site: wikipedia.org, impressions: 14.0 }, 
                 |  { site: live.com, impressions: 1.0 },
                 |  { site: amazon.com, impressions: 1.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="google-s2-geometry-cell-id-transformer">Google S2 Geometry Cell Id Transformer</h3>

<p>The S2 Geometry Library is a spherical geometry library, very useful for manipulating regions on the sphere (commonly on Earth) 
and indexing geographic data. Basically it assigns unique cell id for each region on the earth. </p>

<blockquote>
  <p>Good article about S2 library: <a href="http://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/">Google’s S2, geometry on the sphere, cells and Hilbert curve</a></p>
</blockquote>

<p>For example you can combine S2 transformer with Gather to get from <code>lat</code>/<code>lon</code> to <code>K-V</code> pairs, where key will be <code>S2</code> cell id.
Depending on a level you can assign all people in Greater New York area (level = 4) into one cell, or you can index them block by block (level = 12).</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Transform lat/lon into S2 Cell Id</span>
val s2Transformer = <span class="keyword">new</span> S2CellTransformer()
  .setLevel(<span class="integer">5</span>)
  .setCellCol(<span class="error">“</span>s2_cell<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Gather S2 CellId log</span>
val gatherS2Cells = <span class="keyword">new</span> Gather()
  .setPrimaryKeyCols(<span class="error">“</span>cookie<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val gatheredCells = gatherS2Cells.transform(s2Transformer.transform(geoDf))
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { s2_cell: d5dgds, impressions: 5.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 1.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { s2_cell: d5dgds, impressions: 12.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 3.0 },
                 |  { s2_cell: g7aeg3, impressions: 5.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="assembling-feature-vector">Assembling Feature Vector</h2>

<p><code>K-V</code> pairs from result of <code>Gather</code> are cool, and groups all the information about cookie into single row, however they can’t be used
as input for machine learning. To be able to train a model, predictors data needs to be represented as a vector of doubles. If all features are continuous and
numeric it’s easy, but if some of them are categorical or in <code>gathered</code> shape, it’s not trivial.</p>

<h3 id="gather-encoder">Gather Encoder</h3>

<p>Encodes categorical key-value pairs using dummy variables. </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Encode S2 Cell data</span>
val encodeS2Cells = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setCover(<span class="float">0.95</span>) <span class="comment">// dimensionality reduction</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { s2_cell: d5dgds, impressions: 5.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 1.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { s2_cell: d5dgds, impressions: 12.0 }, 
                 |  { s2_cell: g7aeg3, impressions: 5.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Transformed into</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells Features
—————–|————————
wKgQaV0lHZanDrp  | [ 5.0  ,  1.0 , 0   ]
rfTZLbQDwbu5mXV  | [ 12.0 ,  0   , 5.0 ]
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Note that it’s 3 unique cell id values, that gives 3 columns in final feature vector.</p>

<p>Optionally apply dimensionality reduction using <code>top</code> transformation:</p>

<ul>
  <li>Top coverage, is selecting categorical values by computing the count of distinct users for each value,
sorting the values in descending order by the count of users, and choosing the top values from the resulting
list such that the sum of the distinct user counts over these values covers c percent of all users,
for example, selecting top sites covering 99% of users.</li>
</ul>

<h2 id="spark-ml-pipelines">Spark ML Pipelines</h2>

<p>Spark ML Pipeline - is new high level API for Spark MLLib. </p>

<blockquote>
  <p>A practical ML pipeline often involves a sequence of data pre-processing, feature extraction, model fitting, and validation stages. For example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. <a href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html">Read More.</a> </p>
</blockquote>

<p>In Spark ML it’s possible to split ML pipeline in multiple independent stages, group them together in single pipeline and run it
with Cross Validation and Parameter Grid to find best set of parameters.</p>

<h3 id="put-it-all-together-with-spark-ml-pipelines">Put It All together with Spark ML Pipelines</h3>

<p>Gather encoder is a natural fit into Spark ML Pipeline API.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Encode site data</span>
val encodeSites = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>sites<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>sites_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>site<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Encode S2 Cell data</span>
val encodeS2Cells = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setCover(<span class="float">0.95</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Assemble feature vectors together</span>
val assemble = <span class="keyword">new</span> VectorAssembler()
  .setInputCols(<span class="predefined-type">Array</span>(<span class="error">“</span>sites_f<span class="error">”</span>, <span class="error">“</span>s2_cells_f<span class="error">”</span>))
  .setOutputCol(<span class="error">“</span>features<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Build logistic regression</span>
val lr = <span class="keyword">new</span> LogisticRegression()
  .setFeaturesCol(<span class="error">“</span>features<span class="error">”</span>)
  .setLabelCol(<span class="error">“</span>response<span class="error">”</span>)
  .setProbabilityCol(<span class="error">“</span>probability<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Define pipeline with 4 stages</span>
val pipeline = <span class="keyword">new</span> Pipeline()
  .setStages(<span class="predefined-type">Array</span>(encodeSites, encodeS2Cells, assemble, lr))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val evaluator = <span class="keyword">new</span> BinaryClassificationEvaluator()
  .setLabelCol(Response.response)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val crossValidator = <span class="keyword">new</span> CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val paramGrid = <span class="keyword">new</span> ParamGridBuilder()
  .addGrid(lr.elasticNetParam, <span class="predefined-type">Array</span>(<span class="float">0.1</span>, <span class="float">0.5</span>))
  .build()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;crossValidator.setEstimatorParamMaps(paramGrid)
crossValidator.setNumFolds(<span class="integer">2</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;println(s<span class="error">”</span>Train model on train set<span class="error">”</span>)
val cvModel = crossValidator.fit(trainSet)
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="conclusion">Conclusion</h2>

<p>New Spark ML API makes machine learning much more easier. <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> is good example of how is it possible to 
create custom transformers/estimators that later can be used as a part of bigger pipeline, and can be easily shared/reused by multiple projects.</p>

<blockquote>
  <p>Full code for example application is available on <a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala">Github</a>.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Feature Engineering at Scale With Spark]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/"/>
    <updated>2015-06-10T20:02:45-07:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Check Model Matrix <a href="http://collectivemedia.github.io/modelmatrix/">Website</a> and <a href="https://github.com/collectivemedia/modelmatrix">Github project</a>.</p>
</blockquote>

<p>At <a href="http://collective.com">Collective</a> we are in programmatic advertisement business, it means that all our
advertisement decisions (what ad to show, to whom and at what time) are driven by models. We do a lot of 
machine learning, build thousands predictive models and use them to make millions decision per second.</p>

<h4 id="how-do-we-get-the-most-out-of-our-data-for-predictive-modeling">How do we get the most out of our data for predictive modeling?</h4>

<p>Success of all Machine Learning algorithms depends on data that you put into it, the better the features you choose, the
better the results you will achieve.</p>

<blockquote>
  <p>Feature Engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work better.</p>
</blockquote>

<p>In Ad-Tech it’s finite pieces of information about users that we can put into our models, and it’s 
almost the same across all companies in industry, we don’t have access to any anonymous data
like real name and age, interests on Facebook etc. It really matter how creative you are to get maximum from the data you have,
and how fast you can iterate and test new idea.</p>

<p>In 2014 Collective data science team published <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper that
describes our approach and trade-offs for audience optimization. In 2015 we solve the same problems, but
using new technologies (Spark and Spark MLLib) at even bigger scale. I want to show the tool that I built specifically 
to handle feature engineering/selection problem, and which is open sources now.</p>

<h2 id="model-matrix">Model Matrix</h2>

<!-- more -->

<h3 id="feature-transformation">Feature Transformation</h3>

<p>Imagine impression log that is used to train predictive model</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
visitor_id  | ad_campaign     | ad_id | ad_ctr     | pub_site            | state | city         | price | timestamp   &lt;br /&gt;
———– | ————— | —– | ———- | ——————- | —– | ———— | —– | ————- 
bob         | Nike_Sport      | 1     | 0.01       | http://bbc.com      | NY    | New York     | 0.17  | 1431032702135&lt;br /&gt;
bill        | Burgers_Co      | 2     | 0.005      | http://cnn.com      | CA    | Los Angeles  | 0.42  | 1431032705167 
mary        | Macys           | 3     | 0.015      | http://fashion.com  | CA    | Los Angeles  | 0.19  | 1431032708384 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Producing a feature vector for every visitor (cookie) row and every piece of information about a 
visitor as an p-size vector, where p is the number of predictor variables multiplied by cardinality 
of each variable (number of states in US, number of unique websites, etc …). It is impractical 
both from the data processing standpoint and because the resulting vector would only have 
about 1 in 100,000 non-zero elements.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
 visitor_id  | Nike_Sport | Burgers_Co | Macys | NY  | CA  | … 
 ———– | ———- | ———- | —– | — | — | — 
 bob         | 1.0        |            |       | 1.0 |     | … 
 bill        |            | 1.0        |       |     | 1.0 | … 
 mary        |            |            | 1.0   |     | 1.0 | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Model Matrix uses feature transformations (top, index, binning) to reduce dimensionality to arrive 
at between one and two thousand predictor variables, with data sparsity of about 1 in 10. It removes 
irrelevant and low frequency predictor values from the model, and transforms continuous 
variable into bins of the same size.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre> &lt;br /&gt;
 visitor_id  | Nike | OtherAd | NY  | OtherState | price ∈ [0.01, 0.20) | price ∈ [0.20, 0.90) | … 
 ———– | —- | ——- | — | ———- | ——————– | ——————– | — 
 bob         | 1.0  |         | 1.0 |            | 1.0                  |                      | … 
 bill        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
 mary        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Transformation definitions in scala:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Absence of transformation
 */</span>
<span class="keyword">case</span> object <span class="predefined-type">Identity</span> <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find top values
 * by a quantity that cumulatively cover a given percentage
 * of this quantity. For example, find the top DMAs that
 * represent 99% of cookies, or find top sites that
 * are responsible for 90% of impressions.
 *
 * @param cover      cumulative cover percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Top</span>(<span class="key">cover</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find the values
 * with at least the minimum support in the data set.
 * Support for a value is defined as the percentage of a
 * total quantity that have that value. For example,
 * find segments that appear for at least 1% of the cookies.
 *
 * @param support    support percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Index</span>(<span class="key">support</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Break the values in the column into bins with roughly the same number of points.
 *
 * @param nbins target number of bins
 * @param minPoints minimum number of points in single bin
 * @param minPercents minimum percent of points in a bin (0-100).
 *                    The larger of absolute number and percent points is used.
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Bins</span>(<span class="key">nbins</span>: Int, <span class="key">minPoints</span>: Int = <span class="integer">0</span>, <span class="key">minPercents</span>: <span class="predefined-type">Double</span> = <span class="float">0.0</span>) <span class="directive">extends</span> Transform
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="transformed-columns">Transformed Columns</h3>

<h4 id="categorical-transformation">Categorical Transformation</h4>

<p>A column calculated by applying top or index transform function, each columns id corresponds 
to one unique value from input data set. SourceValue is encoded as ByteVector unique value from 
input column and used later for featurization. </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">CategoricalTransformer</span>(
  <span class="key">features</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(features) {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[CategoricalColumn]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait CategoricalColumn {
  <span class="keyword">def</span> <span class="key">columnId</span>: Int
  <span class="keyword">def</span> <span class="key">count</span>: <span class="predefined-type">Long</span>
  <span class="keyword">def</span> <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object CategoricalColumn {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">CategoricalValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">sourceName</span>: <span class="predefined-type">String</span>,
    <span class="key">sourceValue</span>: ByteVector,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">AllOther</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="bin-column">Bin Column</h4>

<p>A column calculated by applying binning transform function.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">BinsTransformer</span>(
  <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(input) with Binner {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[BinColumn] = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">BinValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">low</span>: <span class="predefined-type">Double</span>,
    <span class="key">high</span>: <span class="predefined-type">Double</span>,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">sampleSize</span>: <span class="predefined-type">Long</span>
  ) 
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="building-model-matrix-instance">Building Model Matrix Instance</h3>

<p>Model Matrix instance contains information about shape of the training data, what transformations (categorical and binning)
are required to apply to input data in order to obtain feature vector that will got into machine learning
algorithm.</p>

<p>Building model matrix instance described well in <a href="http://collectivemedia.github.io/modelmatrix/doc/cli.html">command line interface documentation</a>.</p>

<h3 id="featurizing-your-data">Featurizing your data</h3>

<p>When you have model matrix instance, you can apply it to multiple input data sets. For example in Collective
we build model matrix instance once a week or even month, and use it for building models from daily/hourly data.
It gives us nice property: all models have the same columns, and it’s easy to compare them.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Similar to Spark LabeledPoint</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">IdentifiedPoint</span>(<span class="key">id</span>: Any, <span class="key">features</span>: <span class="predefined-type">Vector</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="type">class</span> <span class="class">Featurization</span>(<span class="key">features</span>: Seq[ModelInstanceFeature]) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Check that all input features belong to the same model instance</span>
  <span class="directive">private</span> val instances = features.map(_.modelInstanceId).toSet
  require(instances.size == <span class="integer">1</span>, 
    s<span class="error">”</span>Features belong to different model <span class="key">instances</span>: <span class="error">$</span>instances<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Maximum columns id in instance features</span>
  <span class="directive">private</span> val totalNumberOfColumns = features.flatMap {
    <span class="keyword">case</span> ModelInstanceIdentityFeature(&lt;em&gt;, _, _, _, columnId) =&amp;gt; Seq(columnId)
    <span class="keyword">case</span> ModelInstanceTopFeature(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;, &lt;em&gt;, _, _, cols) =&amp;gt; cols.map(&lt;</span><span class="delimiter">/</span></span>em&gt;.columnId)
    <span class="keyword">case</span> ModelInstanceIndexFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>    <span class="keyword">case</span> ModelInstanceBinsFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>  }.max&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
   * Featurize input dataset
   *
   * @return id data type and featurized rows
   */</span>
  <span class="keyword">def</span> <span class="function">featurize</span>(
    <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> FeaturesWithId, 
    <span class="key">idColumn</span>: <span class="predefined-type">String</span>
  ): (DataType, RDD[IdentifiedPoint]) = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Extract features from input DataFrame with id column: </span><span class="inline"><span class="inline-delimiter">$</span>idColumn</span><span class="content">. </span><span class="delimiter">&quot;</span></span> + 
         s<span class="string"><span class="delimiter">&quot;</span><span class="content">Total number of columns: </span><span class="inline"><span class="inline-delimiter">$</span>totalNumberOfColumns</span><span class="delimiter">&quot;</span></span>)

...
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;}
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="results">Results</h3>

<p>Model Matrix is open sourced, and available on <a href="https://github.com/collectivemedia/modelmatrix">Github</a>, lot’s of 
documentation on <a href="http://collectivemedia.github.io/modelmatrix/">Website</a>.</p>

<p>We use it at <a href="http://collective.com">Collective</a> to define our models and it works for us really well.</p>

<p>You can continue your reading with <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper, 
to get more data science focused details about our modeling approach.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stock Price Prediction With Big Data and Machine Learning]]></title>
    <link href="http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning/"/>
    <updated>2014-11-14T18:03:35-08:00</updated>
    <id>http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning</id>
    <content type="html"><![CDATA[<p>Apache Spark and Spark MLLib for building price movement prediction model from order log data.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/orderbook-dynamics">Github</a></p>
</blockquote>

<h3 id="synopsis">Synopsis</h3>

<p>This post is based on <a href="https://raw.github.com/ezhulenev/scala-openbook/master/assets/Modeling-high-frequency-limit-order-book-dynamics-with-support-vector-machines.pdf">Modeling high-frequency limit order book dynamics with support vector machines</a> paper.
Roughly speaking I’m implementing ideas introduced in this paper in scala with <a href="https://spark.apache.org/">Spark</a> and <a href="https://spark.apache.org/mllib/">Spark MLLib</a>.
Authors are using sampling, I’m going to use full order log from <a href="http://www.nyxdata.com/Data-Products/NYSE-OpenBook-History">NYSE</a> (sample data is available from <a href="ftp://ftp.nyxdata.com/Historical%20Data%20Samples/TAQ%20NYSE%20OpenBook/">NYSE FTP</a>), just because
I can easily do it with Spark. Instead of using SVM, I’m going to use <a href="http://spark.apache.org/docs/latest/mllib-decision-tree.html">Decision Tree</a> algorithm for classification,
because in Spark MLLib it supports multiclass classification out of the box.</p>

<p>If you want to get deep understanding of the problem and proposed solution, you need to read the paper.
I’m going to give high level overview of the problem in less academic language, in one or two paragraphs.</p>

<blockquote>
  <p>Predictive modelling is the process by which a model is created or chosen to try to best predict the probability of an outcome.</p>
</blockquote>

<!-- more -->

<h4 id="model-architecture">Model Architecture</h4>

<p>Authors are proposing framework for extracting feature vectors from from raw order log data, that can be used as input to
machine learning classification method (SVM or Decision Tree for example) to predict price movement (Up, Down, Stationary). Given a set of training data
with assigned labels (price movement) classification algorithm builds a model that assigns new examples into one of pre-defined categories.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Time(sec)            Price($)   Volume      Event Type      Direction
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
34203.011926972      598.68     10          submission      ask
34203.011926973      594.47     15          submission      bid
34203.011926974      594.49     20          submission      bid
34203.011926981      597.68     30          submission      ask
34203.011926991      594.47     15          execution       ask
34203.011927072      597.68     10          cancellation    ask
34203.011927082      599.88     12          submission      ask
34203.011927097      598.38     11          submission      ask
</pre></div>
</div>
 </figure></notextile></div></p>

<p>In the table, each row of the message book represents a trading event that could be either a order submission,
order cancellation, or order execution. The arrival time measured from midnight,
is in seconds and nanoseconds; price is in US dollars, and the Volume is in number of shares.
Ask - I’m selling and asking for this price, Bid - I want to buy for this price.</p>

<p>From this log it’s very easy to reconstruct state of order book after each entry. You can read more about <a href="http://www.investopedia.com/terms/o/order-book.asp">order book</a>
and <a href="http://www.investopedia.com/university/intro-to-order-types/limit-orders.asp">limit order book</a> in Investopedia,
I’m not going to dive into details. Concepts are super easy for understanding.</p>

<blockquote>
  <p>An electronic list of buy and sell orders for a specific security or financial instrument, organized by price level.</p>
</blockquote>

<h4 id="feature-extraction-and-training-data-preparation">Feature Extraction and Training Data Preparation</h4>

<p>After order books are reconstructed from order log, we can derive attributes, that will form feature vectors used as input to <code>classification model</code>.</p>

<p>Attributes are divided into three categories: basic, time-insensitive, and time-sensitive, all of which can be directly computed from the data.
Attributes in the basic set are the prices and volumes at both ask and bid sides up to n = 10 different levels (that is, price levels in the order book at a given moment),
which can be directly fetched from the order book. Attributes in the time-insensitive set are easily computed from the basic set at a single point in time.
Of this, bid-ask spread and mid-price, price ranges, as well as average price and volume at different price levels are calculated in feature sets <code>v2</code>, <code>v3</code>, and <code>v5</code>, respectively;
while <code>v5</code> is designed to track the accumulated differences of price and volume between ask and bid sides. By further taking the recent history of current data into consideration,
we devise the features in the time-sensitive set. More about calculating other attributes can be found in <a href="https://raw.github.com/ezhulenev/scala-openbook/master/assets/Modeling-high-frequency-limit-order-book-dynamics-with-support-vector-machines.pdf">original paper</a>.</p>

<p><img class="center" src="https://raw.github.com/ezhulenev/scala-openbook/master/assets/features.png"></p>

<h4 id="labeling-training-data">Labeling Training Data</h4>

<p>To prepare training data for machine learning it’s also required to label each point with price movement observed over some time horizon (1 second fo example).
It’s straightforward task that only requires two order books: current order book and order book after some period of time.</p>

<p>I’m going to use <code>MeanPriceMove</code> label that can be: <code>Stationary</code>, <code>Up</code> or <code>Down</code>.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;trait <span class="predefined-type">Label</span>[L] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { label =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">current</span>: OrderBook, <span class="key">future</span>: OrderBook): <span class="predefined-type">Option</span>[L]
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;sealed trait MeanPriceMove&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object MeanPriceMove {
  <span class="keyword">case</span> object Up <span class="directive">extends</span> MeanPriceMove
  <span class="keyword">case</span> object Down <span class="directive">extends</span> MeanPriceMove
  <span class="keyword">case</span> object Stationary <span class="directive">extends</span> MeanPriceMove
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object MeanPriceMovementLabel <span class="directive">extends</span> <span class="predefined-type">Label</span>[MeanPriceMove] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span>[<span class="local-variable">this</span>] val basicSet = BasicSet.apply(BasicSet.Config.default)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">apply</span>(<span class="key">current</span>: OrderBook, <span class="key">future</span>: OrderBook): <span class="predefined-type">Option</span>[MeanPriceMove] = {
    val currentMeanPrice = basicSet.meanPrice(current)
    val futureMeanPrice = basicSet.meanPrice(future)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val <span class="key">cell</span>: Cell[MeanPriceMove] =
   currentMeanPrice.zipMap(futureMeanPrice) {
    (currentMeanValue, futureMeanValue) =&amp;gt;
      <span class="keyword">if</span> (currentMeanValue == futureMeanValue)
        MeanPriceMove.Stationary
      <span class="keyword">else</span> <span class="keyword">if</span> (currentMeanValue &amp;gt; futureMeanValue)
        MeanPriceMove.Down
      <span class="keyword">else</span>
        MeanPriceMove.Up
    }

cell.toOption   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h3 id="order-log-data">Order Log Data</h3>

<p>I’m going to use <a href="http://www.nyxdata.com/Data-Products/NYSE-OpenBook-History">NYSE TAQ OpenBook</a> orders data, and parse it with <a href="https://github.com/ezhulenev/scala-openbook">Scala OpenBook</a>
library. It’s easiest data set to get, free sample data for 2 trading days is available for download at <a href="ftp://ftp.nyxdata.com/Historical%20Data%20Samples/TAQ%20NYSE%20OpenBook/">NYSE FTP</a>.</p>

<blockquote>
  <p>TAQ (Trades and Quotes) historical data products provide a varying range of market depth on a T+1 basis for covered markets.
TAQ data products are used to develop and backtest trading strategies, analyze market trends as seen in a real-time ticker plant environment, and research markets for regulatory or audit activity.</p>
</blockquote>

<h3 id="prepare-training-data">Prepare Training Data</h3>

<p><code>OrderBook</code> is two sorted maps, where key is price and value is volume.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">OrderBook</span>(<span class="key">symbol</span>: <span class="predefined-type">String</span>,
                     <span class="key">buy</span>: <span class="predefined-type">TreeMap</span>[Int, Int] = <span class="predefined-type">TreeMap</span>.empty,
                     <span class="key">sell</span>: <span class="predefined-type">TreeMap</span>[Int, Int] = <span class="predefined-type">TreeMap</span>.empty)
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="feature-sets">Feature Sets</h4>

<p>I’m using <code>Cell</code> from <a href="https://github.com/pellucidanalytics/framian">Framian</a> library to represent extracted feature values. It can be <code>Value</code>, <code>NA</code> or <code>NM</code>.</p>

<p>As defined in original paper we have three feature sets, first two calculated from <code>OrderBook</code>, last one requires <code>OrdersTrail</code> which effectively is
window computation over raw order log.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait <span class="predefined-type">BasicAttribute</span>[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: BasicAttribute[T2] = new BasicAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T2] = self(orderBook).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait TimeInsensitiveAttribute[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: TimeInsensitiveAttribute[T2] = new TimeInsensitiveAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T2] = self(orderBook).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
trait TimeSensitiveAttribute[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">ordersTrail</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: TimeSensitiveAttribute[T2] = new TimeSensitiveAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">ordersTrail</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): Cell[T2] = self(ordersTrail).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>and it’s how features calculation looks like</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">BasicSet</span> <span class="directive">private</span>[attribute] (val <span class="key">config</span>: BasicSet.Config) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {
  <span class="directive">private</span>[attribute] <span class="keyword">def</span> <span class="function">askPrice</span>(<span class="key">orderBook</span>: OrderBook)(<span class="key">i</span>: Int): Cell[Int] = {
    Cell.fromOption {
      orderBook.sell.keySet.drop(i - <span class="integer">1</span>).headOption
    }
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span>[attribute] <span class="keyword">def</span> <span class="function">bidPrice</span>(<span class="key">orderBook</span>: OrderBook)(<span class="key">i</span>: Int): Cell[Int] = {
    Cell.fromOption {
      val bidPrices = orderBook.buy.keySet
      <span class="keyword">if</span> (bidPrices.size &amp;gt;= i) {
        bidPrices.drop(bidPrices.size - i).headOption
      } <span class="keyword">else</span> None
    }
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> <span class="keyword">def</span> attribute&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: OrderBook =&amp;gt; Cell[T]</span><span class="delimiter">&quot;</span></span>&gt;T&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: BasicAttribute[T] = new BasicAttribute[T] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T] = f(orderBook)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">askPrice</span>(<span class="key">i</span>: Int): <span class="predefined-type">BasicAttribute</span>[Int] = attribute(askPrice(_)(i))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">bidPrice</span>(<span class="key">i</span>: Int): <span class="predefined-type">BasicAttribute</span>[Int] = attribute(bidPrice(_)(i))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">meanPrice</span>: <span class="predefined-type">BasicAttribute</span>[<span class="predefined-type">Double</span>] = {
    val ask1 = askPrice(<span class="integer">1</span>)
    val bid1 = bidPrice(<span class="integer">1</span>)
    <span class="predefined-type">BasicAttribute</span>.from(orderBook =&amp;gt;
      ask1(orderBook).zipMap(bid1(orderBook)) {
        (ask, bid) =&amp;gt; (ask.toDouble + bid.toDouble) / <span class="integer">2</span>
      })
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="label-training-data">Label Training Data</h4>

<p>To extract labeled data from orders I’m using <code>LabeledPointsExtractor</code></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">LabeledPointsExtractor</span>[<span class="key">L</span>: LabelEncode] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">labeledPoints</span>(<span class="key">orders</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): <span class="predefined-type">Vector</span>[LabeledPoint] = {
    log.debug(s<span class="error">”</span>Extract labeled points from orders log. Log <span class="key">size</span>: <span class="error">$</span>{orders.size}<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;<span class="comment">// ...   } } </span></pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<p>and it can be constructed nicely with builder</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val extractor = {
    <span class="keyword">import</span> <span class="include">com.scalafi.dynamics.attribute.LabeledPointsExtractor._</span>
    (LabeledPointsExtractor.newBuilder()
      += basic(&lt;em&gt;.askPrice(<span class="integer">1</span>))
      += basic(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.bidPrice(1))</span></span><span class="error">
</span>      += basic(_.meanPrice)
      ).result(symbol, MeanPriceMovementLabel, LabeledPointsExtractor.Config(<span class="integer">1</span>.millisecond))
  }
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This <code>extractor</code> will prepare labeled points using <code>MeanPriceMovementLabel</code> with 3 features: ask price, bid price and mean price</p>

<h3 id="run-classification-model">Run Classification Model</h3>

<p>In “real” application I’m using 36 features from all 3 feature sets. I run my tests with sample data from NYSE ftp,
<code>EQY_US_NYSE_BOOK_20130403</code> for model training and <code>EQY_US_NYSE_BOOK_20130404</code> for model validation.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
object DecisionTreeDynamics <span class="directive">extends</span> App with ConfiguredSparkContext with FeaturesExtractor {
  <span class="directive">private</span> val log = LoggerFactory.getLogger(<span class="local-variable">this</span>.getClass)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">Config</span>(<span class="key">training</span>: <span class="predefined-type">String</span> = <span class="error">“</span><span class="error">”</span>,
                    <span class="key">validation</span>: <span class="predefined-type">String</span> = <span class="error">“</span><span class="error">”</span>,
                    <span class="key">filter</span>: <span class="predefined-type">Option</span>[<span class="predefined-type">String</span>] = None,
                    <span class="key">symbol</span>: <span class="predefined-type">Option</span>[<span class="predefined-type">String</span>] = None)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val parser = <span class="keyword">new</span> OptionParser&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">&amp;quot;Order Book Dynamics&amp;quot;</span><span class="delimiter">&quot;</span></span>&gt;Config&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt; {</span></span><span class="error">
</span>    <span class="comment">// ….</span>
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;parser.parse(args, Config()) map { implicit config =&amp;gt;
    val trainingFiles = openBookFiles(<span class="error">“</span>Training<span class="error">”</span>, config.training, config.filter)
    val validationFiles = openBookFiles(<span class="error">“</span>Validation<span class="error">”</span>, config.validation, config.filter)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val trainingOrderLog = orderLog(trainingFiles)
log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Training order log size: </span><span class="inline"><span class="inline-delimiter">${</span>trainingOrderLog.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)

<span class="comment">// Configure DecisionTree model</span>
val labelEncode = implicitly[LabelEncode[MeanPriceMove]]
val numClasses = labelEncode.numClasses
val categoricalFeaturesInfo = <span class="predefined-type">Map</span>.empty[Int, Int]
val impurity = <span class="string"><span class="delimiter">&quot;</span><span class="content">gini</span><span class="delimiter">&quot;</span></span>
val maxDepth = <span class="integer">5</span>
val maxBins = <span class="integer">100</span>

val trainingData = trainingOrderLog.extractLabeledData(featuresExtractor(<span class="key">_</span>: <span class="predefined-type">String</span>))
val trainedModels = (trainingData map { <span class="keyword">case</span> LabeledOrderLog(symbol, labeledPoints) =&amp;gt;
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Train Decision Tree model. Training data size: </span><span class="inline"><span class="inline-delimiter">${</span>labeledPoints.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
  val model = DecisionTree.trainClassifier(labeledPoints, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)
  val labelCounts = labeledPoints.map(_.label).countByValue().map {
    <span class="keyword">case</span> (key, count) =&amp;gt; (labelEncode.decode(key.toInt), count)
  }
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Label counts: [</span><span class="inline"><span class="inline-delimiter">${</span>labelCounts.mkString(<span class="string"><span class="delimiter">&quot;</span><span class="content">, </span><span class="delimiter">&quot;</span></span>)<span class="inline-delimiter">}</span></span><span class="content">]</span><span class="delimiter">&quot;</span></span>)
  symbol -&amp;gt; model
}).toMap

val validationOrderLog = orderLog(validationFiles)
log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Validation order log size: </span><span class="inline"><span class="inline-delimiter">${</span>validationOrderLog.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
val validationData = validationOrderLog.extractLabeledData(featuresExtractor(<span class="key">_</span>: <span class="predefined-type">String</span>))

<span class="comment">// Evaluate model on validation data and compute training error</span>
validationData.map { <span class="keyword">case</span> LabeledOrderLog(symbol, labeledPoints) =&amp;gt;

  val model = trainedModels(symbol)

  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Evaluate model on validation data. Validation data size: </span><span class="inline"><span class="inline-delimiter">${</span>labeledPoints.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Learned classification tree model: </span><span class="inline"><span class="inline-delimiter">$</span>model</span><span class="delimiter">&quot;</span></span>)

  val labelAndPrediction = labeledPoints.map { point =&amp;gt;
    val prediction = model.predict(point.features)
    (point.label, prediction)
  }
  val trainingError = labelAndPrediction.filter(r =&amp;gt; r._1 != r._2).count().toDouble / labeledPoints.count
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Training Error = </span><span class="delimiter">&quot;</span></span> + trainingError)
}   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h4 id="training-error">Training Error</h4>

<p>Output of running Decision Tree classification for single symbol <code>ORCL</code>:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
ORCL: Train Decision Tree model. Training data size: 64064
ORCL: Trained model in 3740 millis
ORCL: Label counts: [Stationary -&amp;gt; 42137, Down -&amp;gt; 10714, Up -&amp;gt; 11213]
ORCL: Evaluate model on validation data. Validation data size: 54749
ORCL: Training Error = 0.28603262160039455
</pre></div>
</div>
 </figure></notextile></div></p>

<p>As you can see this pretty simple model was able to successfully classify ~70% of the data.</p>

<p><strong>Remark:</strong> Despite the fact, that this model shows very good success rate, it doesn’t mean that it
can be successfully used to build profitable automated trading strategy. First of all I don’t check
if it’s 95% success predicting stationary and 95% error rate predicting any price movement with
average 70% success rate. It doesn’t measure “strength” of price movement, it has to be sufficient to cover
transaction costs. And many other details that matters for building real trading system.</p>

<p>For sure it’s huge room for improvement and result validation. Unfortunately it’s hard do get enough data,
2 trading days is to small data set to draw conclusions and start building system to earn all the money in the world.
However I think it’ a good starting point.</p>

<h3 id="results">Results</h3>

<p>I was able to relatively easy reproduce fairly complicated research project at much lager scale than in original paper.</p>

<p>Latest Big Data technologies allows to build models using all available data, and stop doing samplings.
Using all of the data helps to build best possible models and capture all details from full data set.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/orderbook-dynamics">Github</a></p>
</blockquote>
]]></content>
  </entry>
  
</feed>
