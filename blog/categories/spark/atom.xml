<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Eugene Zhulenev]]></title>
  <link href="http://eugenezhulenev.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://eugenezhulenev.com/"/>
  <updated>2015-12-03T14:41:42-05:00</updated>
  <id>http://eugenezhulenev.com/</id>
  <author>
    <name><![CDATA[Eugene Zhulenev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark in Production: Lessons From Running Large Scale Machine Learning]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/12/03/spark-in-production-large-scale-machine-learning/"/>
    <updated>2015-12-03T13:24:46-05:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/12/03/spark-in-production-large-scale-machine-learning</id>
    <content type="html"><![CDATA[<p>I wrote earlier about our approach for <a href="/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines">machine learning with Spark</a> 
at <a href="http://collective.com">Collective</a>, it was focused on transforming raw data into features that can be used for training a model.
At this post I want describe how to assemble multiple building blocks into production application, that efficiently uses
Spark cluster and can train/validate hundreds of models.</p>

<p>Training single model is relatively easy, and it’s well covered in Spark documentation and multiple other blog posts. Training hundreds of 
models can become really tricky from engineering point of view. Spark has lot’s of configuration parameters 
that can affect cluster performance and stability, and you can use some clever tricks to get higher cluster utilization.</p>

<!-- more -->

<h3 id="scale-of-the-problem">Scale Of The Problem</h3>

<p>At Collective we are using Spark and machine learning for online advertising optimization, trying to decide which ads are relevant to 
which people, at which time and at which web site. </p>

<p>Log data used for training models is huge, billions of rows. Number of users that we target is hundreds of millions. 
We have hundreds of clients with tens of different campaigns, with different optimization targets and restrictions.</p>

<p>These factors gives an idea of the scale of the problem.  </p>

<h3 id="production-machine-learning-pipeline">Production Machine Learning Pipeline</h3>

<p>Typical machine learning pipeline for one specific ad campaign looks like this:</p>

<ul>
  <li><em>Prepare response dataset:</em> based on impression/activity logs define which users should be in positive set (did some actions that we are trying to optimize for, example could be signing up for test drive)</li>
  <li><em>Prepare train dataset:</em> extract predictors data from logs</li>
  <li><em>Feauturize:</em> given predictors data extract feature vectors</li>
  <li><em>Train model:</em> feed features and response data into Spark ML</li>
  <li><em>Prepare test dataset:</em> data that is going to be used for model performance evaluation</li>
  <li><em>Evaluate model:</em> for binary classification it can be computing ROC and AUC</li>
</ul>

<p>Preparing datasets is usually contains of multiple join and filter conditions. Featurization and training built on top 
of <a href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html">Spark ML Pipeline</a> API, and 
compose multiple transformers and estimators together. It’s covered in <a href="/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines">one of previous posts</a>.</p>

<p>Given different nature of each step they have different limiting factors. Preparing dataset is expensive operation and heavily 
uses shuffle. Model training time is usually dominated by network latency introduced by iterative function optimization algorithm.</p>

<p>Serial execution of these steps can’t efficiently utilize all executor cores. Running multiple models in parallel in our 
case doesn’t really help as well, multiple models reach <em>modeling</em> stage at almost the same time.</p>

<h3 id="introducing-scalaz-stream">Introducing Scalaz-Stream</h3>

<p>I’ll just put first paragraph from amazing <a href="https://gist.github.com/djspiewak/d93a9c4983f63721c41c">Introduction to scalaz-stream</a> here:</p>

<blockquote>
  <p>Every application ever written can be viewed as some sort of transformation on data. Data can come from different sources, such as a network or a file or user input or the Large Hadron Collider. It can come from many sources all at once to be merged and aggregated in interesting ways, and it can be produced into many different output sinks, such as a network or files or graphical user interfaces. You might produce your output all at once, as a big data dump at the end of the world (right before your program shuts down), or you might produce it more incrementally. Every application fits into this model.</p>
</blockquote>

<p>We model machine learning pipeline as a <code>scalaz.stream.Process</code> - multistep transformation on data, and use <code>scalaz-stream</code> combinators to run it
with controlled concurrency and resource safety.</p>

<p>Simple domain model for campaign optimization can be defined like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">OptimizedCampaign</span>(<span class="key">id</span>: Int, <span class="key">input</span>: Input, <span class="key">target</span>: <span class="predefined-type">Target</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">ModelingError</span>(<span class="key">error</span>: <span class="predefined-type">String</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">TrainedAndEvaluatedModel</span>(<span class="error">…</span>)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Input would be definition of campaign that needs to be optimized, and output would be model that was trained and evaluated or error if something went wrong.</p>

<p>Each of modeling steps described earlier can be encoded as <code>scalaz.stream.Channel</code> transformations:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">import</span> <span class="include">scalaz.stream._</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val prepareResponse = 
  channel.lift[Task, OptimizedCampaign, ModelingError <span class="error">\</span>/ ResponseDataset] {
    <span class="keyword">case</span> opt =&amp;gt;
       <span class="comment">// Expensive join/filter etc…</span>
       val <span class="key">response</span>: DataFrame = 
         dataset1
           .join(dataset2, <span class="error">…</span>)
           .filter(..)
           .select(<span class="error">…</span>)
       ResponseDataset(response)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val prepareTrainDataset = lift[ResponseDataset, TrainDataset] {
  <span class="keyword">case</span> response =&amp;gt;
    <span class="comment">// Another expensive joins that requires shuffle</span>
    val <span class="key">train</span>: DataFrame = 
      dataset1.join(dataset2, <span class="error">…</span>).filter(<span class="error">…</span>)
    TrainDataset(train)&lt;br /&gt;
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val featurize = lift[TrainDataset, FeaturizedDataset] {
  <span class="keyword">case</span> train =&amp;gt; 
     <span class="comment">// Compute featurization using ML Pipeline API</span>
     val pipeline = <span class="keyword">new</span> Pipeline()
       .setStages(<span class="predefined-type">Array</span>(encodeSites, encodeS2Cells, assemble, lr))
     pipeline.fit(train.dataFrame).transform(train.dataFrame)&lt;br /&gt;
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val trainModel = lift[FeaturizedDataset, TrainedModel] {
  <span class="keyword">case</span> featurized =&amp;gt; 
    <span class="comment">// Train model with featurized data </span>
    val lr = <span class="keyword">new</span> LogisticRegression().set(<span class="error">…</span>)
    val pipeline = <span class="keyword">new</span> Pipeline().setStages(<span class="predefined-type">Array</span>(encode, lr))
    val evaluator = <span class="keyword">new</span> BinaryClassificationEvaluator() &lt;br /&gt;
    val crossValidator = <span class="keyword">new</span> CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)  &lt;br /&gt;
    val paramGrid = <span class="keyword">new</span> ParamGridBuilder()
      .addGrid(lr.elasticNetParam, <span class="predefined-type">Array</span>(<span class="float">0.1</span>, <span class="float">0.5</span>))
      .build()  &lt;br /&gt;
    val model = crossValidator.fit(featurized.dataFrame)
    TrainedModel(model)
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val prepareTestDataset = lift[TrainedModel, TestDataset] {
  <span class="keyword">case</span> model =&amp;gt; <span class="error">…</span>
} &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val evaluateModel = lift[TestDataset, TrainedAndEvaluatedModel] {
  <span class="keyword">case</span> test =&amp;gt; <span class="error">…</span>
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Helper method that allows each step to </span>
<span class="comment">// return &lt;code&gt;ModelingError \/ Result&lt;/code&gt; and nicely chains it together</span>
<span class="directive">private</span> <span class="keyword">def</span> lift&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: A =&amp;gt; ModelingError </span><span class="content">\/</span><span class="content"> B</span><span class="delimiter">&quot;</span></span>&gt;A, B&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt; = {</span></span><span class="error">
</span>  channel.lift[Task, ModelingError <span class="error">\</span>/ A, ModelingError <span class="error">\</span>/ B] {
      <span class="keyword">case</span> -<span class="error">\</span><span class="regexp"><span class="delimiter">/</span><span class="content">(err) =&amp;gt; Task.now(</span><span class="char">\/</span><span class="content">.left(err))</span></span><span class="error">
</span>      <span class="keyword">case</span> <span class="error">\</span><span class="regexp"><span class="delimiter">/</span><span class="content">-(a) =&amp;gt; task(f(a))</span></span><span class="error">
</span>    }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Given previously defined modeling steps, optimization pipeline can be defined as <code>Process</code> transformation.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">def</span> <span class="function">optimize</span>(
  <span class="key">campaigns</span>: <span class="predefined-type">Process</span>[Task, OptimizedCampaign]
): <span class="predefined-type">Process</span>[Task, ModelingError <span class="error">\</span>/ TrainedAndEvaluatedModel] = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;campaigns
    .concurrently(<span class="integer">2</span>)(prepareResponse)
    .concurrently(<span class="integer">2</span>)(prepareTrainDataset)
    .concurrently(<span class="integer">2</span>)(featurize)
    .concurrently(<span class="integer">10</span>)(trainModel)
    .concurrently(<span class="integer">2</span>)(prepareTestDataset)
    .concurrently(<span class="integer">2</span>)(evaluateModel)
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>I’m using <code>concurrently</code> method, which runs each step with controlled concurrency. Steps that are doing heavy shuffles 
are running not more than 2 in parallel, in contrast to model training that is relatively lightweight operation and can run with much higher
concurrency. This helper method is described in <a href="/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines">earlier post</a>.</p>

<h4 id="push-vs-pull-based-streams">Push vs Pull Based Streams</h4>

<p><code>scalaz-steam</code> uses pull based model, it means that not first step (prepare response) is pushing data down the transformation chain when it’s ready, but
the bottom step (evaluate model) asks the previous step for new data when it’s done.</p>

<p>This allows to keep Spark cluster always busy, for example when relatively slow running modeling step is done, it 
asks <code>featurize</code> for new data, and it’s already there, which means that modeling can start immediately.</p>

<h3 id="spark-cluster-tuning">Spark Cluster Tuning</h3>

<p>For better cluster utilization I suggest to use <code>FAIR</code> scheduler mode, that can be turned on with <code>--conf spark.scheduler.mode=FAIR</code> flag.</p>

<p>Another big problem for us was tuning garbage collection. I’ve spent a lot of time trying to tune <code>G1</code> collector, but <code>ConcMarkSweepGC</code> with <code>ParNewGC</code> showed the best results
in our case. It doesn’t guarantee that it’s also the best choice for your particular case.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
–conf spark.executor.extraJavaOptions=”-server -XX:+AggressiveOpts -XX:-UseBiasedLocking -XX:NewSize=4g -XX:MaxNewSize=4g -XX:+UseParNewGC -XX:MaxTenuringThreshold=2 -XX:SurvivorRatio=4 -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=32768 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -XX:+PrintGCDetails -XX:+PrintAdaptiveSizePolicy -XX:+PrintTenuringDistribution -XX:+PrintGCDateStamps”
–driver-java-options “-server -XX:+AggressiveOpts -XX:-UseBiasedLocking -XX:NewSize=4g -XX:MaxNewSize=4g -XX:+UseParNewGC -XX:MaxTenuringThreshold=2 -XX:SurvivorRatio=4 -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=32768 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -XX:+PrintGCDetails -XX:+PrintAdaptiveSizePolicy -XX:+PrintTenuringDistribution -XX:+PrintGCDateStamps -Xloggc:gc.log” 
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="streams-everywhere">Streams Everywhere</h3>

<p><code>scalaz-stream</code> is a great abstraction and as it’s described in <a href="https://gist.github.com/djspiewak/d93a9c4983f63721c41c">scalaz-stream Introduction</a> <em>every</em> application <em>can</em>, and I believe <strong>should be</strong> modeled this way.</p>

<p>This approach is embraced not only in scala community, but also in clojure, take a look for example at 
Rich Hickey presentation about <a href="http://www.infoq.com/presentations/clojure-core-async">clojure core.async channels</a>, and how your application can
be modeled with queues.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimizing Spark Machine Learning for Small Data]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/09/16/spark-ml-for-big-and-small-data/"/>
    <updated>2015-09-16T10:04:25-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/09/16/spark-ml-for-big-and-small-data</id>
    <content type="html"><![CDATA[<blockquote>
  <p><strong>Update 2015-10-08</strong>: Optimization “hack” described in this post still works, however we don’t use it in production anymore. 
With careful parallelism config, overhead introduced by distributed models is negligible.</p>
</blockquote>

<p>You’ve all probably already know how awesome is Spark for doing Machine Learning on Big Data. However I’m pretty sure
no one told you how bad (slow) it can be on Small Data. </p>

<p>As I mentioned in my <a href="/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines">previous post</a>, we
extensively use Spark for doing machine learning and audience modeling. It turned out that in some cases, for example when
we are starting optimization for new client/campaign we simply don’t have enough positive examples to construct big enough dataset, so that
using Spark would make sense.</p>

<!-- more -->

<h3 id="spark-ml-from-10000-feet">Spark ML from 10000 feet</h3>

<p>Essentially every machine learning algorithm is a function minimization, where function value depends on some calculation using data in <code>RDD</code>.
For example logistic regression can calculate function value 1000 times before it will converge and find optimal parameters. It means that it will 
compute some <code>RDD</code> 1000 times. In case of <code>LogisticRegression</code> it’s doing <code>RDD.treeAggregate</code> which is supper efficient, but still it’s distributed 
computation.</p>

<p>Now imagine that all the data you have is 50000 rows, and you have for example 1000 partitions. It means that each partition has only 50 rows. And 
each <code>RDD.treeAggregate</code> on every iteration serializing closures, sending them to partitions and collecting result back. 
It’s <strong>HUGE OVERHEAD</strong> and huge load on a driver.</p>

<h3 id="throw-away-spark-and-use-pythonr">Throw Away Spark and use Python/R?</h3>

<p>It’s definitely an option, but we don’t want to build multiple systems for data of different size. Spark ML pipelines are awesome abstraction,
and we want to use it for all machine learning jobs. Also we want to use the same algorithm, so results would be consistent if dataset size
just crossed the boundary between small and big data.</p>

<h3 id="run-logisticregression-in-local-mode">Run LogisticRegression in ‘Local Mode’</h3>

<p>What if Spark could run the same machine learning algorithm, but instead of using <code>RDD</code> for storing input data, it would use <code>Arrays</code>?
It solves all the problems, you get consistent model, computed 10-20x faster because it doesn’t need distributed computations.</p>

<p>That’s exactly approach I used in <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a>, it’s called <a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-mllib/src/main/scala/org/apache/spark/ml/classification/LocalLogisticRegression.scala">LocalLogisticRegression</a>.
It’s mostly copy-pasta from Spark <code>LogisticRegression</code>, but when input data frame has only single partition, it’s running
function optimization on one of the executors using <code>mapPartition</code> function, essentially using Spark as distributed executor service.</p>

<p>This approach is much better than collecting data to driver, because you are not limited by driver computational resources.</p>

<p>When <code>DataFrame</code> has more than 1 partition it just falls back to default distributed logistic regression.</p>

<p>Code for new <code>train</code> method looks like this:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">def</span> <span class="function">trainLocal</span>(
      <span class="key">instances</span>: <span class="predefined-type">Array</span>[(<span class="predefined-type">Double</span>, <span class="predefined-type">Vector</span>)]
    ): (LogisticRegressionModel, <span class="predefined-type">Array</span>[<span class="predefined-type">Double</span>]) = <span class="error">…</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">train</span>(<span class="key">dataset</span>: DataFrame): LogisticRegressionModel = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">if</span> (dataset.rdd.partitions.length == <span class="integer">1</span>) {
    log.info(s<span class="error">”</span>Build LogisticRegression <span class="keyword">in</span> local mode<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val (model, objectiveHistory) = extractLabeledPoints(dataset).map {
  <span class="keyword">case</span> LabeledPoint(<span class="key">label</span>: <span class="predefined-type">Double</span>, <span class="key">features</span>: <span class="predefined-type">Vector</span>) =&amp;gt; (label, features)
}.mapPartitions { instances =&amp;gt;
  Seq(trainLocal(instances.toArray)).toIterator
}.first()

val logRegSummary = <span class="keyword">new</span> BinaryLogisticRegressionTrainingSummary(
  model.transform(dataset),
  probabilityCol,
  labelCol,
  objectiveHistory)
model.setSummary(logRegSummary)
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;} <span class="keyword">else</span> {
    log.info(s<span class="error">”</span>Fallback to distributed LogisticRegression<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val that = classOf[LogisticRegression].getConstructor(classOf[<span class="predefined-type">String</span>]).newInstance(uid)
val logisticRegression = copyValues(that)
<span class="comment">// Scala Reflection magic to call protected train method</span>
...
logisticRegression.train(dataset)   } }       </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<p>If input dataset size is less than 100000 rows, it will be placed inside single partition, and regression model will be trained in local mode.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val <span class="key">base</span>: DataFrame = <span class="error">…</span>
val datasetPartitionSize = <span class="integer">100000</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Compute optimal partitions size based on base join</span>
val baseSize = base.count()
val numPartitions = (baseSize.toDouble / datasetPartitionSize).ceil.toInt
log.debug(s<span class="error">”</span>Response base <span class="key">size</span>: <span class="error">$</span>baseSize<span class="error">”</span>)
log.debug(s<span class="error">”</span>Repartition dataset using <span class="error">$</span>numPartitions partitions<span class="error">”</span>)
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="results">Results</h2>

<p>With a little ingenuity (and copy paste) Spark became perfect tool for machine learning both on Small and Big Data. Most awesome thing is that this
new <code>LocalLogisticRegression</code> can be used as drop in replacement in Spark ML pipelines, producing exactly the same <code>LogisticRegressionModel</code> at the end.</p>

<p>It might be interesting idea to use this approach in Spark itself, because in this case it would be possible to do it
without doing so many code duplication. I’d love to see if anyone else had the same problem, and how solved it.</p>

<blockquote>
  <p>More cool Spark things in <a href="https://github.com/collectivemedia/spark-ext/">Github</a>.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Audience Modeling With Spark ML Pipelines]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines/"/>
    <updated>2015-09-09T09:04:25-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines</id>
    <content type="html"><![CDATA[<p>At <a href="http://collective.com">Collective</a> we are heavily relying on machine learning and predictive modeling to 
run digital advertising business. All decisions about what ad to show at this particular time to this particular user
are made by machine learning models (some of them are real time, and some of them are offline).</p>

<p>We have a lot of projects that uses machine learning, common name for all of them can be <strong>Audience Modeling</strong>, as they
all are trying to predict audience conversion (<em>CTR, Viewability Rate, etc…</em>) based on browsing history, behavioral segments and other type of 
predictors.</p>

<p>For most of new development we use <a href="https://spark.apache.org">Spark</a> and <a href="https://spark.apache.org/mllib/">Spark MLLib</a>. It is a awesome project,
however we found that some nice tools/libraries that are widely used for example in R are missing in Spark. In order to add missing
features that we would really like to have in Spark, we created <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> - Spark Extensions
Library. </p>

<blockquote>
  <p>Spark Ext on Github: <a href="https://github.com/collectivemedia/spark-ext">https://github.com/collectivemedia/spark-ext</a></p>
</blockquote>

<p>I’m going to show simple example of combining <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> with Spark ML pipelines for predicting user conversions based geo and browsing history data.</p>

<blockquote>
  <p>Spark ML pipeline example: <a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala">SparkMlExtExample.scala</a></p>
</blockquote>

<!-- more -->

<h2 id="predictors-data">Predictors Data</h2>

<p>I’m using dataset with 2 classes, that will be used for solving classification problem (user converted or not). It’s created with 
<a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/DataGenerator.scala">dummy data generator</a>, 
so that these 2 classes can be easily separated. It’s pretty similar to real data that usually available in digital advertising.</p>

<h3 id="browsing-history-log">Browsing History Log</h3>

<p>History of web sites that were visited by user.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie          | Site          | Impressions&lt;br /&gt;
————— |————– | ————-
wKgQaV0lHZanDrp | live.com      | 24
wKgQaV0lHZanDrp | pinterest.com | 21
rfTZLbQDwbu5mXV | wikipedia.org | 14
rfTZLbQDwbu5mXV | live.com      | 1
rfTZLbQDwbu5mXV | amazon.com    | 1
r1CSY234HTYdvE3 | youtube.com   | 10
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="geo-location-log">Geo Location Log</h3>

<p>Latitude/Longitude impression history.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie          | Lat     | Lng       | Impressions
————— |———| ——— | ————
wKgQaV0lHZanDrp | 34.8454 | 77.009742 | 13
wKgQaV0lHZanDrp | 31.8657 | 114.66142 | 1
rfTZLbQDwbu5mXV | 41.1428 | 74.039600 | 20
rfTZLbQDwbu5mXV | 36.6151 | 119.22396 | 4
r1CSY234HTYdvE3 | 42.6732 | 73.454185 | 4
r1CSY234HTYdvE3 | 35.6317 | 120.55839 | 5
20ep6ddsVckCmFy | 42.3448 | 70.730607 | 21
20ep6ddsVckCmFy | 29.8979 | 117.51683 | 1
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="transforming-predictors-data">Transforming Predictors Data</h2>

<p>As you can see predictors data (sites and geo) is in <em>long</em> format, each <code>cookie</code> has multiple rows associated with it,
and it’s in general is not a good fit for machine learning.
We’d like <code>cookie</code> to be a primary key, and all other data should form <code>feature vector</code>.</p>

<h3 id="gather-transformer">Gather Transformer</h3>

<p>Inspired by R <code>tidyr</code> and <code>reshape2</code> packages. Convert <em>long</em> <code>DataFrame</code> with values
for each key into <em>wide</em> <code>DataFrame</code>, applying aggregation function if single
key has multiple values.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val gather = <span class="keyword">new</span> Gather()
      .setPrimaryKeyCols(<span class="error">“</span>cookie<span class="error">”</span>)
      .setKeyCol(<span class="error">“</span>site<span class="error">”</span>)
      .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
      .setValueAgg(<span class="error">“</span>sum<span class="error">”</span>)         <span class="comment">// sum impression by key</span>
      .setOutputCol(<span class="error">“</span>sites<span class="error">”</span>)
val gatheredSites = gather.transform(siteLog)    &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | Sites
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { site: live.com, impressions: 24.0 }, 
                 |  { site: pinterest.com, impressions: 21.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { site: wikipedia.org, impressions: 14.0 }, 
                 |  { site: live.com, impressions: 1.0 },
                 |  { site: amazon.com, impressions: 1.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="google-s2-geometry-cell-id-transformer">Google S2 Geometry Cell Id Transformer</h3>

<p>The S2 Geometry Library is a spherical geometry library, very useful for manipulating regions on the sphere (commonly on Earth) 
and indexing geographic data. Basically it assigns unique cell id for each region on the earth. </p>

<blockquote>
  <p>Good article about S2 library: <a href="http://blog.christianperone.com/2015/08/googles-s2-geometry-on-the-sphere-cells-and-hilbert-curve/">Google’s S2, geometry on the sphere, cells and Hilbert curve</a></p>
</blockquote>

<p>For example you can combine S2 transformer with Gather to get from <code>lat</code>/<code>lon</code> to <code>K-V</code> pairs, where key will be <code>S2</code> cell id.
Depending on a level you can assign all people in Greater New York area (level = 4) into one cell, or you can index them block by block (level = 12).</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Transform lat/lon into S2 Cell Id</span>
val s2Transformer = <span class="keyword">new</span> S2CellTransformer()
  .setLevel(<span class="integer">5</span>)
  .setCellCol(<span class="error">“</span>s2_cell<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Gather S2 CellId log</span>
val gatherS2Cells = <span class="keyword">new</span> Gather()
  .setPrimaryKeyCols(<span class="error">“</span>cookie<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val gatheredCells = gatherS2Cells.transform(s2Transformer.transform(geoDf))
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { s2_cell: d5dgds, impressions: 5.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 1.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { s2_cell: d5dgds, impressions: 12.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 3.0 },
                 |  { s2_cell: g7aeg3, impressions: 5.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="assembling-feature-vector">Assembling Feature Vector</h2>

<p><code>K-V</code> pairs from result of <code>Gather</code> are cool, and groups all the information about cookie into single row, however they can’t be used
as input for machine learning. To be able to train a model, predictors data needs to be represented as a vector of doubles. If all features are continuous and
numeric it’s easy, but if some of them are categorical or in <code>gathered</code> shape, it’s not trivial.</p>

<h3 id="gather-encoder">Gather Encoder</h3>

<p>Encodes categorical key-value pairs using dummy variables. </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Encode S2 Cell data</span>
val encodeS2Cells = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setCover(<span class="float">0.95</span>) <span class="comment">// dimensionality reduction</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells
—————–|———————————————-
wKgQaV0lHZanDrp  | [
                 |  { s2_cell: d5dgds, impressions: 5.0 }, 
                 |  { s2_cell: b8dsgd, impressions: 1.0 }
                 | ]
rfTZLbQDwbu5mXV  | [
                 |  { s2_cell: d5dgds, impressions: 12.0 }, 
                 |  { s2_cell: g7aeg3, impressions: 5.0 }
                 | ]
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Transformed into</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Cookie           | S2 Cells Features
—————–|————————
wKgQaV0lHZanDrp  | [ 5.0  ,  1.0 , 0   ]
rfTZLbQDwbu5mXV  | [ 12.0 ,  0   , 5.0 ]
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Note that it’s 3 unique cell id values, that gives 3 columns in final feature vector.</p>

<p>Optionally apply dimensionality reduction using <code>top</code> transformation:</p>

<ul>
  <li>Top coverage, is selecting categorical values by computing the count of distinct users for each value,
sorting the values in descending order by the count of users, and choosing the top values from the resulting
list such that the sum of the distinct user counts over these values covers c percent of all users,
for example, selecting top sites covering 99% of users.</li>
</ul>

<h2 id="spark-ml-pipelines">Spark ML Pipelines</h2>

<p>Spark ML Pipeline - is new high level API for Spark MLLib. </p>

<blockquote>
  <p>A practical ML pipeline often involves a sequence of data pre-processing, feature extraction, model fitting, and validation stages. For example, classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. <a href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html">Read More.</a> </p>
</blockquote>

<p>In Spark ML it’s possible to split ML pipeline in multiple independent stages, group them together in single pipeline and run it
with Cross Validation and Parameter Grid to find best set of parameters.</p>

<h3 id="put-it-all-together-with-spark-ml-pipelines">Put It All together with Spark ML Pipelines</h3>

<p>Gather encoder is a natural fit into Spark ML Pipeline API.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Encode site data</span>
val encodeSites = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>sites<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>sites_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>site<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Encode S2 Cell data</span>
val encodeS2Cells = <span class="keyword">new</span> GatherEncoder()
  .setInputCol(<span class="error">“</span>s2_cells<span class="error">”</span>)
  .setOutputCol(<span class="error">“</span>s2_cells_f<span class="error">”</span>)
  .setKeyCol(<span class="error">“</span>s2_cell<span class="error">”</span>)
  .setValueCol(<span class="error">“</span>impressions<span class="error">”</span>)
  .setCover(<span class="float">0.95</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Assemble feature vectors together</span>
val assemble = <span class="keyword">new</span> VectorAssembler()
  .setInputCols(<span class="predefined-type">Array</span>(<span class="error">“</span>sites_f<span class="error">”</span>, <span class="error">“</span>s2_cells_f<span class="error">”</span>))
  .setOutputCol(<span class="error">“</span>features<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Build logistic regression</span>
val lr = <span class="keyword">new</span> LogisticRegression()
  .setFeaturesCol(<span class="error">“</span>features<span class="error">”</span>)
  .setLabelCol(<span class="error">“</span>response<span class="error">”</span>)
  .setProbabilityCol(<span class="error">“</span>probability<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Define pipeline with 4 stages</span>
val pipeline = <span class="keyword">new</span> Pipeline()
  .setStages(<span class="predefined-type">Array</span>(encodeSites, encodeS2Cells, assemble, lr))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val evaluator = <span class="keyword">new</span> BinaryClassificationEvaluator()
  .setLabelCol(Response.response)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val crossValidator = <span class="keyword">new</span> CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val paramGrid = <span class="keyword">new</span> ParamGridBuilder()
  .addGrid(lr.elasticNetParam, <span class="predefined-type">Array</span>(<span class="float">0.1</span>, <span class="float">0.5</span>))
  .build()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;crossValidator.setEstimatorParamMaps(paramGrid)
crossValidator.setNumFolds(<span class="integer">2</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;println(s<span class="error">”</span>Train model on train set<span class="error">”</span>)
val cvModel = crossValidator.fit(trainSet)
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="conclusion">Conclusion</h2>

<p>New Spark ML API makes machine learning much more easier. <a href="https://github.com/collectivemedia/spark-ext">Spark Ext</a> is good example of how is it possible to 
create custom transformers/estimators that later can be used as a part of bigger pipeline, and can be easily shared/reused by multiple projects.</p>

<blockquote>
  <p>Full code for example application is available on <a href="https://github.com/collectivemedia/spark-ext/blob/master/sparkext-example/src/main/scala/com/collective/sparkext/example/SparkMlExtExample.scala">Github</a>.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interactive Audience Analytics With Spark and HyperLogLog]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog/"/>
    <updated>2015-07-15T22:07:44-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog</id>
    <content type="html"><![CDATA[<p>At <a href="http://collective.com">Collective</a> we are working not only on cool things like 
<a href="/blog/2015/06/10/2015-06-10-feature-engineering-at-scale">Machine Learning and Predictive Modeling</a>, 
but also on reporting that can be tedious and boring. However at our scale even simple reporting 
application can become challenging engineering problem. This post is based on talk that 
I gave at <a href="http://www.meetup.com/ny-scala/events/223751768/">NY-Scala Meetup</a>. Slides are available <a href="/talks/interactive-audience-analytics/">here</a>.</p>

<blockquote>
  <p>Example application is available on github: <a href="https://github.com/collectivemedia/spark-hyperloglog">https://github.com/collectivemedia/spark-hyperloglog</a></p>
</blockquote>

<!-- more -->

<h2 id="impression-log">Impression Log</h2>

<p>We are building reporting application that is based on impression log. It’s not exactly the way how we get data from out partners,
it’s pre-aggregated by Ad, Site, Cookie. And even in this pre-aggregated format it takes hundreds of gigabytes per day on HDFS.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Ad            | Site          | Cookie          | Impressions | Clicks | Segments                     &lt;br /&gt;
————- |————– | ————— | ———– | —— | ——————————-
bmw_X5        | forbes.com    | 13e835610ff0d95 | 10          | 1      | [a.m, b.rk, c.rh, d.sn, …] &lt;br /&gt;
mercedes_2015 | forbes.com    | 13e8360c8e1233d | 5           | 0      | [a.f, b.rk, c.hs, d.mr, …] &lt;br /&gt;
nokia         | gizmodo.com   | 13e3c97d526839c | 8           | 0      | [a.m, b.tk, c.hs, d.sn, …] &lt;br /&gt;
apple_music   | reddit.com    | 1357a253f00c0ac | 3           | 1      | [a.m, b.rk, d.sn, e.gh, …] &lt;br /&gt;
nokia         | cnn.com       | 13b23555294aced | 2           | 1      | [a.f, b.tk, c.rh, d.sn, …] &lt;br /&gt;
apple_music   | facebook.com  | 13e8333d16d723d | 9           | 1      | [a.m, d.sn, g.gh, s.hr, …] &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Each cookie id has assigned segments which are just 4-6 letters code, that represents some information about cookie, that we
get from 3rd party data providers such as <a href="http://www.bluekai.com">Blukai</a>.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
- a.m  : Male
- a.f  : Female
- b.tk : $75k-$100k annual income
- b.rk : $100k-$150k annual income
- c.hs : High School
- c.rh : College
- d.sn : Single
- d.mr : Married
</pre></div>
</div>
 </figure></notextile></div></p>

<p>For example if cookie has assigned <code>a.m</code> segment, it means that we think (actually data provider thinks) that this cookie belongs to male.
The same thing for annual income level. </p>

<p>We don’t have precise information, to whom exactly particular cookie belongs, and what is real
annual income level, this segments are essentially probabilistic, but we can get very interesting insights from this data.</p>

<h3 id="what-we-can-do-with-this-data">What we can do with this data</h3>

<p>Using this impression log we can answer some interesting questions</p>

<ul>
  <li>We can calculate a given group’s prevalence in a campaign’s audience, eg. what role do <strong>males</strong> play in the optimized audience for a <strong>Goodyear Tires</strong> campaign?</li>
  <li>What is <strong>male/female</strong> ratio for people who have seen <strong>bmw_X5</strong> ad on <strong>forbes.com</strong></li>
  <li>Income distribution for people who have seen Apple Music ad</li>
  <li>Nokia click distribution across different education levels  </li>
</ul>

<p>Using this basic questions we can create so called “Audience Profile”, that describes what type of audience is prevailing in optimized campaign or partner web site.</p>

<p><img class="center" src="/talks/interactive-audience-analytics/affinity.png"></p>

<p>Blue bar means that this particular segment tend to view ad/visit web site more than on average, and red bar mean less. For example for <strong>Goodyear Tires</strong> we expect to see
more <strong>male</strong> audience than <strong>female</strong>.</p>

<h2 id="solving-problem-with-sql">Solving problem with SQL</h2>

<p>SQL looks like an easy choice for this problem, however as I already mentioned we have hundreds of gigabytes of data every day, and we
need to get numbers based on 1 year history in seconds. Hive/Impala simply can’t solve this problem.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="class">select</span> <span class="predefined">count</span>(<span class="keyword">distinct</span> cookie_id) <span class="keyword">from</span> impressions
    <span class="keyword">where</span> site = <span class="error">‘</span>forbes.com<span class="error">’</span>
    <span class="keyword">and</span> ad = <span class="error">‘</span>bmw_X5<span class="error">’</span>
    <span class="keyword">and</span> segment contains <span class="error">‘</span>a.m<span class="error">’</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Unfortunately we have almost infinite combinations of filters that users can define, so it’s not feasible to pre-generate all possible reports.
Users can use any arbitrary ad, site, campaign, order filter combinations, and may want to know audience intersection with any segment.</p>

<h2 id="audience-cardinality-approximation-with-hyperloglog">Audience cardinality approximation with HyperLogLog</h2>

<p>We came up with different solution, instead of providing precise results for every query, we are providing approximated number, but with
very high precision. Usually error is around 2% which for this particular application is really good. We don’t need to know exact number of male/female
cookies in audience. To be able to say what audience is prevailing, approximated numbers are more than enough.</p>

<p>We use <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>, which is algorithm for the count-distinct problem, 
approximating the number of distinct elements (cardinality). It uses finite space and has configurable precision. 
It able to estimate cardinalities of &gt;10^9 with a typical accuracy of 2%, using 1.5kB of memory.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
trait HyperLogLog {
    <span class="keyword">def</span> <span class="function">add</span>(<span class="key">cookieId</span>: <span class="predefined-type">String</span>): Unit
    <span class="comment">//   |A|</span>
    <span class="keyword">def</span> <span class="function">cardinality</span>(): <span class="predefined-type">Long</span>
    <span class="comment">//   |A ∪ B|</span>
    <span class="keyword">def</span> <span class="function">merge</span>(<span class="key">other</span>: HyperLogLog): HyperLogLog
    <span class="comment">//   |A ∩ B| = |A| + |B| - |A ∪ B|,</span>
    <span class="keyword">def</span> <span class="function">intersect</span>(<span class="key">other</span>: HyperLogLog): <span class="predefined-type">Long</span>
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Here is roughly API that is provided by <code>HyperLogLog</code>. You can add new cookieId to it, get cardinality estimation of unique cookies that were 
already added to it, merge it with another <code>HyperLogLog</code>, and finally get intersection. It’s important to notice that
after <code>intersect</code> operation <code>HyperLogLog</code> object is lost, and you have only approximated intersection cardinality. 
So usually <code>HyperLogLog</code> intersection is the last step in computation.</p>

<p>I suggest you to watch awesome talk by <a href="https://twitter.com/avibryant">Avi Bryant</a> where he discusses not only HyperLogLog but lot’s of other
approximation data structures that can be useful for big-data analytics: <a href="http://www.infoq.com/presentations/abstract-algebra-analytics">http://www.infoq.com/presentations/abstract-algebra-analytics</a>.</p>

<h2 id="from-cookies-to-hyperloglog">From cookies to HyperLogLog</h2>

<p>We split out original impression log into two tables. </p>

<p>For ad impressions table we remove segment information and aggregate cookies, impressions and clicks by Ad and Site. <code>HyperLogLog</code> can 
be used in aggregation function exactly the same was as <code>sum</code> operation. Zero is empty <code>HyperLogLog</code>, and plus operation is <code>merge</code> (btw it’s exactly
properties required by <code>Monoid</code>)</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Ad            | Site          | Cookies HLL        | Impressions | Clicks 
————- | ————- | —————— | ———– | —— 
bmw_X5        | forbes.com    | HyperLogLog@23sdg4 | 5468        | 35   &lt;br /&gt;
bmw_X5        | cnn.com       | HyperLogLog@84jdg4 | 8943        | 29   &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p>For segments table we remove ad and site information, and aggregate data by segment.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Segment       | Cookies HLL        | Impressions | Clicks
————- | —————— | ———– | ——
Male          | HyperLogLog@85sdg4 | 235468      | 335 &lt;br /&gt;
$100k-$150k   | HyperLogLog@35jdg4 | 569473      | 194 &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="percent-of-college-and-high-school-education-in-bmw-campaign">Percent of college and high school education in BMW campaign</h3>

<p>If you imaging that we can load these tables into <code>Seq</code>, then audience intersection becomes really straightforward task, that can
be solved in couple line of functional scala operations.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Audience</span>(<span class="key">ad</span>: <span class="predefined-type">String</span>, <span class="key">site</span>: <span class="predefined-type">String</span>, <span class="key">hll</span>: HyperLogLog, <span class="key">imp</span>: <span class="predefined-type">Long</span>, <span class="key">clk</span>: <span class="predefined-type">Long</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">Segment</span>(<span class="key">name</span>: <span class="predefined-type">String</span>, <span class="key">hll</span>: HyperLogLog, <span class="key">imp</span>: <span class="predefined-type">Long</span>, <span class="key">clk</span>: <span class="predefined-type">Long</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">adImpressions</span>: Seq[Audience] = <span class="error">…</span>
val <span class="key">segmentImpressions</span>: Seq[<span class="predefined-type">Segment</span>] = <span class="error">…</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">bmwCookies</span>: HyperLogLog = adImpressions
    .filter(&lt;em&gt;.ad = <span class="error">“</span>bmw_X5<span class="error">”</span>)
    .map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.hll).reduce(_ merge _)&lt;</span><span class="delimiter">/</span></span>p&gt;

&lt;p&gt;val <span class="key">educatedCookies</span>: HyperLogLog = segmentImpressions
    .filter(&lt;em&gt;.segment <span class="keyword">in</span> Seq(<span class="error">“</span>College<span class="error">”</span>, <span class="error">“</span>High School<span class="error">”</span>))
    .map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.hll).reduce( _ merge _)&lt;</span><span class="delimiter">/</span></span>p&gt;

&lt;p&gt;val p = (bmwCookies intersect educatedCookies) / bmwCookies.count()
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="spark-dataframes-with-hyperloglog">Spark DataFrames with HyperLogLog</h2>

<p>Obviously we can’t load all the data into scala <code>Seq</code> on single machine, because it’s huge, even after removing cookie level data
and transforming it into <code>HyperLogLog</code> objects, it’s around 1-2 gigabytes of data for single day.</p>

<p>So we have to use some distributed data processing framework to solve this problem, and we chose Spark.</p>

<h3 id="what-is-spark-dataframe">What is Spark DataFrame</h3>

<ul>
  <li>Inspired by R data.frame and Python/Pandas DataFrame</li>
  <li>Distributed collection of rows organized into named columns</li>
  <li>Used to be SchemaRDD in Spark &lt; 1.3.0</li>
</ul>

<h3 id="high-level-dataframe-operations">High-Level DataFrame Operations</h3>

<ul>
  <li>Selecting required columns</li>
  <li>Filtering</li>
  <li>Joining different data sets</li>
  <li>Aggregation (count, sum, average, etc)</li>
</ul>

<p>You can start from <a href="https://spark.apache.org/docs/1.3.0/sql-programming-guide.html">Spark DataFrame guide</a> or <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">DataBricks blog post</a>.</p>

<h3 id="ad-impressions-and-segments-in-dataframes">Ad impressions and segments in DataFrames</h3>

<p>We store all out data on HDFS using Parquet data format, and that’s how it looks after it’s loaded into Spark DataFrame.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val <span class="key">adImpressions</span>: DataFrame = sqlContext.parquetFile(<span class="error">“</span><span class="regexp"><span class="delimiter">/</span><span class="content">aa</span><span class="delimiter">/</span></span>audience<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;adImpressions.printSchema()
<span class="comment">// root</span>
<span class="comment">//   | – ad: string (nullable = true)</span>
<span class="comment">//   | – site: string (nullable = true)</span>
<span class="comment">//   | – hll: binary (nullable = true)</span>
<span class="comment">//   | – impressions: long (nullable = true)</span>
<span class="comment">//   | – clicks: long (nullable = true)&lt;/p&gt;</span>

&lt;p&gt;val <span class="key">segmentImpressions</span>: DataFrame = sqlContext.parquetFile(<span class="error">“</span><span class="regexp"><span class="delimiter">/</span><span class="content">aa</span><span class="delimiter">/</span></span>segments<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;segmentImpressions.printSchema()
<span class="comment">// root</span>
<span class="comment">//   | – segment: string (nullable = true)</span>
<span class="comment">//   | – hll: binary (nullable = true)</span>
<span class="comment">//   | – impressions: long (nullable = true)</span>
<span class="comment">//   | – clicks: long (nullable = true)</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p><code>HyperLogLog</code> is essentially huge <code>Array[Byte]</code> with some clever hashing and math, so it’s straightforward to store it on HDFS in serialized form.</p>

<h2 id="working-with-spark-dataframe">Working with Spark DataFrame</h2>

<p>We want to know answer for the same question: “Percent of college and high school education in BMW campaign”.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">import</span> <span class="include">org.apache.spark.sql.functions._</span>
<span class="keyword">import</span> <span class="include">org.apache.spark.sql.HLLFunctions._</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">bmwCookies</span>: HyperLogLog = adImpressions
    .filter(col(<span class="error">“</span>ad<span class="error">”</span>) === <span class="error">“</span>bmw_X5<span class="error">”</span>)
    .select(mergeHll(col(<span class="error">“</span>hll<span class="error">”</span>)).first() <span class="comment">// – sum(clicks)&lt;/p&gt;</span>

&lt;p&gt;val <span class="key">educatedCookies</span>: HyperLogLog = hllSegments
    .filter(col(<span class="error">“</span>segment<span class="error">”</span>) <span class="keyword">in</span> Seq(<span class="error">“</span>College<span class="error">”</span>, <span class="error">“</span>High School<span class="error">”</span>))
    .select(mergeHll(col(<span class="error">“</span>hll<span class="error">”</span>)).first()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val p = (bmwCookies intersect educatedCookies) / bmwCookies.count()
</pre></div>
</div>
 </figure></notextile></div></p>

<p>It looks pretty familiar, not too far from example based on scala <code>Seq</code>. Only one unusual operation, that you might notice if you have some
experience with Spark is <code>mergeHLL</code>. It’s not available in Spark by default, it’s custom <code>PartialAggregate</code> function that can compute aggregates
for serialized <code>HyperLogLog</code> objects.</p>

<h3 id="writing-your-own-spark-aggregation-function">Writing your own Spark aggregation function</h3>

<p>To write you own aggregation function you need to define function that will be applied to each row in <code>RDD</code> partition, in this example
it’s called <code>MergeHLLPartition</code>. Then you need to define function that will take results from different partitions and merge them together, for <code>HyperLogLog</code>
it’s called <code>MergeHLLMerge</code>. And finally you need to tell Spark how you want it to split your computation across <code>RDD</code> (DataFrame is backed by <code>RDD[Row]</code>) </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLLPartition</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> AggregateExpression with trees.UnaryNode[<span class="predefined-type">Expression</span>] { <span class="error">…</span> }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLLMerge</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> AggregateExpression with trees.UnaryNode[<span class="predefined-type">Expression</span>] { <span class="error">…</span> }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLL</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> PartialAggregate with trees.UnaryNode[<span class="predefined-type">Expression</span>] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override <span class="keyword">def</span> <span class="key">asPartial</span>: SplitEvaluation = {
    val partial = Alias(MergeHLLPartition(child), <span class="error">“</span>PartialMergeHLL<span class="error">”</span>)()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;SplitEvaluation(
  MergeHLLMerge(partial.toAttribute),
  partial :: Nil
)   } }
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;<span class="keyword">def</span> <span class="function">mergeHLL</span>(<span class="key">e</span>: Column): Column = MergeHLL(e.expr)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>After that writing aggregations becomes really easy task, and your expressions will look like “native” DataFrame code, which is really nice, and super
easy to read and reason about. </p>

<p>Also it works much faster then solving this problem with scala transformations on top of <code>RDD[Row]</code>, as Spark catalyst optimizer can executed optimized
plan and reduce amount of data that needs to be shuffled between spark nodes.</p>

<p>And finally it’s so much easier to manage mutable state. Spark encourage you to use immutable transformations, and it’s really cool until you need
extreme performance from your code. For example if you are using something like <code>reduce</code> or <code>aggregateByKey</code> you don’t really know when and where
your function instantiated and when it’s done with <code>RDD</code> partition and result transferred to another Spark node for merge operation. With <code>AggregateExpression</code> 
you have explicit control over mutable state, and it’s totally safe to accumulate mutable state during execution for single partition, and at the end when
you’ll need to send data to other node you can create immutable copy.</p>

<p>In this particular case using mutable <code>HyperLogLog</code> merge implementation helped to speed up computation time almost 10x times. For each partition <code>HyperLogLog</code> state
accumulated in single mutable <code>Array[Byte]</code> and at the end when data needs to be transferred somewhere else for merging with another partition, immutable copy is created.</p>

<h3 id="some-fancy-aggregates-with-dataframe-api">Some fancy aggregates with DataFrame Api</h3>

<p>You can write much more complicated aggregation functions, for example to compute aggregate based on multiple columns. Here is code sample from 
our audience analytics project.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">SegmentEstimate</span>(<span class="key">cookieHLL</span>: HyperLogLog, <span class="key">clickHLL</span>: HyperLogLog)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;type SegmentName = <span class="predefined-type">String</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">dailyEstimates</span>: RDD[(SegmentName, <span class="predefined-type">Map</span>[LocalDate, SegmentEstimate])] =
    segments.groupBy(segment_name).agg(
      segment_name,
      mergeDailySegmentEstimates(
        mkDailySegmentEstimate(      <span class="comment">// – Map[LocalDate, SegmentEstimate]</span>
          dt,
          mkSegmentEstimate(         <span class="comment">// – SegmentEstimate(cookieHLL, clickHLL)</span>
            cookie_hll,
            click_hll)
        )
      )
    )
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This codes computes daily audience aggregated by segment. Using Spark <code>PartialAggregate</code> function 
saves a lot of network traffic and minimizes distributed shuffle size. </p>

<p>This aggregation is possible because of nice properties of <code>Monoid</code></p>

<ul>
  <li><code>HyperLogLog</code> is a <code>Monoid</code> (has <code>zero</code> and <code>plus</code> operations)</li>
  <li><code>SegmentEstimate</code> is a <code>Monoid</code> (tuple of two monoids)</li>
  <li><code>Map[K, SegmentEstimate]</code> is a <code>Monoid</code> (map with value monoid value type is monoid itself)</li>
</ul>

<h3 id="problems-with-custom-aggregation-functions">Problems with custom aggregation functions</h3>

<ul>
  <li>Right now it’s closed API, so you need to place all your code under <code>org.apache.spark.sql</code> package.</li>
  <li>It’s no guarantee that it will work in next Spark release.</li>
  <li>If you want to try, I suggest you to start with <code>org.apache.spark.sql.catalyst.expressions.Sum</code> as example.</li>
</ul>

<h2 id="spark-as-in-memory-sql-database">Spark as in-memory SQL database</h2>

<p>We use Spark as in-memory database that serves SQL (composed with DataFrame Api) queries. </p>

<p>People tend to think about spark with very batch oriented mindset. Start Spark cluster in Yarn, do computation, kill cluster. Submit you application to 
standalone Spark cluster (Mesos), kill it. Biggest problem with this approach that after your application is done, and JVM is killed, <code>SparkContext</code> is lost,
and even if you are running Spark in standalone mode, all data cached by your application is lost.</p>

<p>We use Spark in totally different way. We start Spark cluster in Yarn, load data to it from HDFS, cache it in memory, and <strong>do not shutdown</strong>. We
keep JVM running, it holds a reference to <code>SparkContext</code> and keeps all the data in memory on worker nodes.</p>

<p>Our backend application is essentially very simpre REST/JSON server built with Spray, that holds <code>SparkContext</code> reference, receive requests via
URL parameters, runs queries in Spark and return response in JSON.</p>

<p>Right now (July 2015) we have data starting from April, and it’s around 100g cached in 40 nodes. We need to keep 1 year history, so we don’t expect
more than 500g. And we are very confident that we can scale horizontally without seriously affecting performance. Right now average 
request response time is 1-2 seconds which is really good for our use case.</p>

<h2 id="spark-best-practices">Spark Best practices</h2>

<p>Here are configuration options that I found really useful for our specific task. You can find more details about each of them in Spark guide.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
- spark.scheduler.mode=FAIR
- spark.yarn.executor.memoryOverhead=4000
- spark.sql.autoBroadcastJoinThreshold=300000000 // ~300mb
- spark.serializer=org.apache.spark.serializer.KryoSerializer
- spark.speculation=true
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Also I found that it’s really important to repartition your dataset if you are going to cache it and use for queries. Optimal number of partitions is
around 4-6 for each executor core, with 40 nodes and 6 executor cores we use 1000 partitions for best performance.</p>

<p>If you have too many partitions Spark will spend too much time for coordination, and receiving results from all partitions. If too small, you might have
problems with too big block during shuffle that can kill not only performance, but all your cluster: <a href="https://issues.apache.org/jira/browse/SPARK-1476">SPARK-1476</a></p>

<h2 id="other-options">Other Options</h2>

<p>Before starting this project we were evaluating some other options</p>

<h3 id="hive">Hive</h3>

<p>Obviously it’s too slow for interactive UI backend, but we found it really useful for batch data processing. We use it to process raw logs
and build aggregated tables with <code>HyperLogLog</code> inside.</p>

<h3 id="impala">Impala</h3>

<p>To get good performance out of Impala it’s required to write C++ user defined functions, and it’s was not the task that I wanted to do. Also 
I’m not confident that even with custom C++ function Impala can show performance that we need.</p>

<h4 id="druid">Druid</h4>

<p><a href="http://druid.io/">Druid</a> is really interesting project, and it’s used in another project at Collective for slightly different problem, 
but it’s not in production yet.</p>

<ul>
  <li>Managing separate Druid cluster - it’s not the task that I want to do</li>
  <li>We have batch oriented process - and druid data ingestion is stream based</li>
  <li>Bad support for some of type of queries that we need - if I need to know intersection of some particular ad with all segments, in case of druid it will be 10k (number of segments) queries, and it will obviously fail to complete in 1-2 seconds </li>
  <li>Not clear how get data back from Druid - it’s hard to get data back from Druid later, if it will turn out that it doesn’t solve out problems well</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Spark is Awesome! I didn’t have any major issues with it, and it just works! New DataFrame API is amazing, and we are going to build lot’s of new cool projects at Collective
with Spar MLLib and GraphX, and I’m pretty sure they all will be successful.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Feature Engineering at Scale With Spark]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/"/>
    <updated>2015-06-10T23:02:45-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Check Model Matrix <a href="http://collectivemedia.github.io/modelmatrix/">Website</a> and <a href="https://github.com/collectivemedia/modelmatrix">Github project</a>.</p>
</blockquote>

<p>At <a href="http://collective.com">Collective</a> we are in programmatic advertisement business, it means that all our
advertisement decisions (what ad to show, to whom and at what time) are driven by models. We do a lot of 
machine learning, build thousands predictive models and use them to make millions decision per second.</p>

<h4 id="how-do-we-get-the-most-out-of-our-data-for-predictive-modeling">How do we get the most out of our data for predictive modeling?</h4>

<p>Success of all Machine Learning algorithms depends on data that you put into it, the better the features you choose, the
better the results you will achieve.</p>

<blockquote>
  <p>Feature Engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work better.</p>
</blockquote>

<p>In Ad-Tech it’s finite pieces of information about users that we can put into our models, and it’s 
almost the same across all companies in industry, we don’t have access to any anonymous data
like real name and age, interests on Facebook etc. It really matter how creative you are to get maximum from the data you have,
and how fast you can iterate and test new idea.</p>

<p>In 2014 Collective data science team published <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper that
describes our approach and trade-offs for audience optimization. In 2015 we solve the same problems, but
using new technologies (Spark and Spark MLLib) at even bigger scale. I want to show the tool that I built specifically 
to handle feature engineering/selection problem, and which is open sources now.</p>

<h2 id="model-matrix">Model Matrix</h2>

<!-- more -->

<h3 id="feature-transformation">Feature Transformation</h3>

<p>Imagine impression log that is used to train predictive model</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
visitor_id  | ad_campaign     | ad_id | ad_ctr     | pub_site            | state | city         | price | timestamp   &lt;br /&gt;
———– | ————— | —– | ———- | ——————- | —– | ———— | —– | ————- 
bob         | Nike_Sport      | 1     | 0.01       | http://bbc.com      | NY    | New York     | 0.17  | 1431032702135&lt;br /&gt;
bill        | Burgers_Co      | 2     | 0.005      | http://cnn.com      | CA    | Los Angeles  | 0.42  | 1431032705167 
mary        | Macys           | 3     | 0.015      | http://fashion.com  | CA    | Los Angeles  | 0.19  | 1431032708384 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Producing a feature vector for every visitor (cookie) row and every piece of information about a 
visitor as an p-size vector, where p is the number of predictor variables multiplied by cardinality 
of each variable (number of states in US, number of unique websites, etc …). It is impractical 
both from the data processing standpoint and because the resulting vector would only have 
about 1 in 100,000 non-zero elements.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
 visitor_id  | Nike_Sport | Burgers_Co | Macys | NY  | CA  | … 
 ———– | ———- | ———- | —– | — | — | — 
 bob         | 1.0        |            |       | 1.0 |     | … 
 bill        |            | 1.0        |       |     | 1.0 | … 
 mary        |            |            | 1.0   |     | 1.0 | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Model Matrix uses feature transformations (top, index, binning) to reduce dimensionality to arrive 
at between one and two thousand predictor variables, with data sparsity of about 1 in 10. It removes 
irrelevant and low frequency predictor values from the model, and transforms continuous 
variable into bins of the same size.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre> &lt;br /&gt;
 visitor_id  | Nike | OtherAd | NY  | OtherState | price ∈ [0.01, 0.20) | price ∈ [0.20, 0.90) | … 
 ———– | —- | ——- | — | ———- | ——————– | ——————– | — 
 bob         | 1.0  |         | 1.0 |            | 1.0                  |                      | … 
 bill        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
 mary        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Transformation definitions in scala:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Absence of transformation
 */</span>
<span class="keyword">case</span> object <span class="predefined-type">Identity</span> <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find top values
 * by a quantity that cumulatively cover a given percentage
 * of this quantity. For example, find the top DMAs that
 * represent 99% of cookies, or find top sites that
 * are responsible for 90% of impressions.
 *
 * @param cover      cumulative cover percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Top</span>(<span class="key">cover</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find the values
 * with at least the minimum support in the data set.
 * Support for a value is defined as the percentage of a
 * total quantity that have that value. For example,
 * find segments that appear for at least 1% of the cookies.
 *
 * @param support    support percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Index</span>(<span class="key">support</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Break the values in the column into bins with roughly the same number of points.
 *
 * @param nbins target number of bins
 * @param minPoints minimum number of points in single bin
 * @param minPercents minimum percent of points in a bin (0-100).
 *                    The larger of absolute number and percent points is used.
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Bins</span>(<span class="key">nbins</span>: Int, <span class="key">minPoints</span>: Int = <span class="integer">0</span>, <span class="key">minPercents</span>: <span class="predefined-type">Double</span> = <span class="float">0.0</span>) <span class="directive">extends</span> Transform
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="transformed-columns">Transformed Columns</h3>

<h4 id="categorical-transformation">Categorical Transformation</h4>

<p>A column calculated by applying top or index transform function, each columns id corresponds 
to one unique value from input data set. SourceValue is encoded as ByteVector unique value from 
input column and used later for featurization. </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">CategoricalTransformer</span>(
  <span class="key">features</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(features) {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[CategoricalColumn]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait CategoricalColumn {
  <span class="keyword">def</span> <span class="key">columnId</span>: Int
  <span class="keyword">def</span> <span class="key">count</span>: <span class="predefined-type">Long</span>
  <span class="keyword">def</span> <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object CategoricalColumn {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">CategoricalValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">sourceName</span>: <span class="predefined-type">String</span>,
    <span class="key">sourceValue</span>: ByteVector,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">AllOther</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="bin-column">Bin Column</h4>

<p>A column calculated by applying binning transform function.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">BinsTransformer</span>(
  <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(input) with Binner {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[BinColumn] = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">BinValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">low</span>: <span class="predefined-type">Double</span>,
    <span class="key">high</span>: <span class="predefined-type">Double</span>,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">sampleSize</span>: <span class="predefined-type">Long</span>
  ) 
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="building-model-matrix-instance">Building Model Matrix Instance</h3>

<p>Model Matrix instance contains information about shape of the training data, what transformations (categorical and binning)
are required to apply to input data in order to obtain feature vector that will got into machine learning
algorithm.</p>

<p>Building model matrix instance described well in <a href="http://collectivemedia.github.io/modelmatrix/doc/cli.html">command line interface documentation</a>.</p>

<h3 id="featurizing-your-data">Featurizing your data</h3>

<p>When you have model matrix instance, you can apply it to multiple input data sets. For example in Collective
we build model matrix instance once a week or even month, and use it for building models from daily/hourly data.
It gives us nice property: all models have the same columns, and it’s easy to compare them.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Similar to Spark LabeledPoint</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">IdentifiedPoint</span>(<span class="key">id</span>: Any, <span class="key">features</span>: <span class="predefined-type">Vector</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="type">class</span> <span class="class">Featurization</span>(<span class="key">features</span>: Seq[ModelInstanceFeature]) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Check that all input features belong to the same model instance</span>
  <span class="directive">private</span> val instances = features.map(_.modelInstanceId).toSet
  require(instances.size == <span class="integer">1</span>, 
    s<span class="error">”</span>Features belong to different model <span class="key">instances</span>: <span class="error">$</span>instances<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Maximum columns id in instance features</span>
  <span class="directive">private</span> val totalNumberOfColumns = features.flatMap {
    <span class="keyword">case</span> ModelInstanceIdentityFeature(&lt;em&gt;, _, _, _, columnId) =&amp;gt; Seq(columnId)
    <span class="keyword">case</span> ModelInstanceTopFeature(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;, &lt;em&gt;, _, _, cols) =&amp;gt; cols.map(&lt;</span><span class="delimiter">/</span></span>em&gt;.columnId)
    <span class="keyword">case</span> ModelInstanceIndexFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>    <span class="keyword">case</span> ModelInstanceBinsFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>  }.max&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
   * Featurize input dataset
   *
   * @return id data type and featurized rows
   */</span>
  <span class="keyword">def</span> <span class="function">featurize</span>(
    <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> FeaturesWithId, 
    <span class="key">idColumn</span>: <span class="predefined-type">String</span>
  ): (DataType, RDD[IdentifiedPoint]) = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Extract features from input DataFrame with id column: </span><span class="inline"><span class="inline-delimiter">$</span>idColumn</span><span class="content">. </span><span class="delimiter">&quot;</span></span> + 
         s<span class="string"><span class="delimiter">&quot;</span><span class="content">Total number of columns: </span><span class="inline"><span class="inline-delimiter">$</span>totalNumberOfColumns</span><span class="delimiter">&quot;</span></span>)

...
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;}
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="results">Results</h3>

<p>Model Matrix is open sourced, and available on <a href="https://github.com/collectivemedia/modelmatrix">Github</a>, lot’s of 
documentation on <a href="http://collectivemedia.github.io/modelmatrix/">Website</a>.</p>

<p>We use it at <a href="http://collective.com">Collective</a> to define our models and it works for us really well.</p>

<p>You can continue your reading with <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper, 
to get more data science focused details about our modeling approach.</p>
]]></content>
  </entry>
  
</feed>
