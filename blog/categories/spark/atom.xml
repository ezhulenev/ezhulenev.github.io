<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Eugene Zhulenev]]></title>
  <link href="http://eugenezhulenev.com/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://eugenezhulenev.com/"/>
  <updated>2015-09-08T13:34:09-04:00</updated>
  <id>http://eugenezhulenev.com/</id>
  <author>
    <name><![CDATA[Eugene Zhulenev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Interactive Audience Analytics With Spark and HyperLogLog]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog/"/>
    <updated>2015-07-15T22:07:44-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog</id>
    <content type="html"><![CDATA[<p>At <a href="http://collective.com">Collective</a> we are working not only on cool things like 
<a href="/blog/2015/06/10/2015-06-10-feature-engineering-at-scale">Machine Learning and Predictive Modeling</a>, 
but also on reporting that can be tedious and boring. However at our scale even simple reporting 
application can become challenging engineering problem. This post is based on talk that 
I gave at <a href="http://www.meetup.com/ny-scala/events/223751768/">NY-Scala Meetup</a>. Slides are available <a href="/talks/interactive-audience-analytics/">here</a>.</p>

<blockquote>
  <p>Example application is available on github: <a href="https://github.com/collectivemedia/spark-hyperloglog">https://github.com/collectivemedia/spark-hyperloglog</a></p>
</blockquote>

<!-- more -->

<h2 id="impression-log">Impression Log</h2>

<p>We are building reporting application that is based on impression log. It’s not exactly the way how we get data from out partners,
it’s pre-aggregated by Ad, Site, Cookie. And even in this pre-aggregated format it takes hundreds of gigabytes per day on HDFS.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Ad            | Site          | Cookie          | Impressions | Clicks | Segments                     &lt;br /&gt;
————- |————– | ————— | ———– | —— | ——————————-
bmw_X5        | forbes.com    | 13e835610ff0d95 | 10          | 1      | [a.m, b.rk, c.rh, d.sn, …] &lt;br /&gt;
mercedes_2015 | forbes.com    | 13e8360c8e1233d | 5           | 0      | [a.f, b.rk, c.hs, d.mr, …] &lt;br /&gt;
nokia         | gizmodo.com   | 13e3c97d526839c | 8           | 0      | [a.m, b.tk, c.hs, d.sn, …] &lt;br /&gt;
apple_music   | reddit.com    | 1357a253f00c0ac | 3           | 1      | [a.m, b.rk, d.sn, e.gh, …] &lt;br /&gt;
nokia         | cnn.com       | 13b23555294aced | 2           | 1      | [a.f, b.tk, c.rh, d.sn, …] &lt;br /&gt;
apple_music   | facebook.com  | 13e8333d16d723d | 9           | 1      | [a.m, d.sn, g.gh, s.hr, …] &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Each cookie id has assigned segments which are just 4-6 letters code, that represents some information about cookie, that we
get from 3rd party data providers such as <a href="http://www.bluekai.com">Blukai</a>.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
- a.m  : Male
- a.f  : Female
- b.tk : $75k-$100k annual income
- b.rk : $100k-$150k annual income
- c.hs : High School
- c.rh : College
- d.sn : Single
- d.mr : Married
</pre></div>
</div>
 </figure></notextile></div></p>

<p>For example if cookie has assigned <code>a.m</code> segment, it means that we think (actually data provider thinks) that this cookie belongs to male.
The same thing for annual income level. </p>

<p>We don’t have precise information, to whom exactly particular cookie belongs, and what is real
annual income level, this segments are essentially probabilistic, but we can get very interesting insights from this data.</p>

<h3 id="what-we-can-do-with-this-data">What we can do with this data</h3>

<p>Using this impression log we can answer some interesting questions</p>

<ul>
  <li>We can calculate a given group’s prevalence in a campaign’s audience, eg. what role do <strong>males</strong> play in the optimized audience for a <strong>Goodyear Tires</strong> campaign?</li>
  <li>What is <strong>male/female</strong> ratio for people who have seen <strong>bmw_X5</strong> ad on <strong>forbes.com</strong></li>
  <li>Income distribution for people who have seen Apple Music ad</li>
  <li>Nokia click distribution across different education levels  </li>
</ul>

<p>Using this basic questions we can create so called “Audience Profile”, that describes what type of audience is prevailing in optimized campaign or partner web site.</p>

<p><img class="center" src="/talks/interactive-audience-analytics/affinity.png"></p>

<p>Blue bar means that this particular segment tend to view ad/visit web site more than on average, and red bar mean less. For example for <strong>Goodyear Tires</strong> we expect to see
more <strong>male</strong> audience than <strong>female</strong>.</p>

<h2 id="solving-problem-with-sql">Solving problem with SQL</h2>

<p>SQL looks like an easy choice for this problem, however as I already mentioned we have hundreds of gigabytes of data every day, and we
need to get numbers based on 1 year history in seconds. Hive/Impala simply can’t solve this problem.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="class">select</span> <span class="predefined">count</span>(<span class="keyword">distinct</span> cookie_id) <span class="keyword">from</span> impressions
    <span class="keyword">where</span> site = <span class="error">‘</span>forbes.com<span class="error">’</span>
    <span class="keyword">and</span> ad = <span class="error">‘</span>bmw_X5<span class="error">’</span>
    <span class="keyword">and</span> segment contains <span class="error">‘</span>a.m<span class="error">’</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Unfortunately we have almost infinite combinations of filters that users can define, so it’s not feasible to pre-generate all possible reports.
Users can use any arbitrary ad, site, campaign, order filter combinations, and may want to know audience intersection with any segment.</p>

<h2 id="audience-cardinality-approximation-with-hyperloglog">Audience cardinality approximation with HyperLogLog</h2>

<p>We came up with different solution, instead of providing precise results for every query, we are providing approximated number, but with
very high precision. Usually error is around 2% which for this particular application is really good. We don’t need to know exact number of male/female
cookies in audience. To be able to say what audience is prevailing, approximated numbers are more than enough.</p>

<p>We use <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>, which is algorithm for the count-distinct problem, 
approximating the number of distinct elements (cardinality). It uses finite space and has configurable precision. 
It able to estimate cardinalities of &gt;10^9 with a typical accuracy of 2%, using 1.5kB of memory.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
trait HyperLogLog {
    <span class="keyword">def</span> <span class="function">add</span>(<span class="key">cookieId</span>: <span class="predefined-type">String</span>): Unit
    <span class="comment">//   |A|</span>
    <span class="keyword">def</span> <span class="function">cardinality</span>(): <span class="predefined-type">Long</span>
    <span class="comment">//   |A ∪ B|</span>
    <span class="keyword">def</span> <span class="function">merge</span>(<span class="key">other</span>: HyperLogLog): HyperLogLog
    <span class="comment">//   |A ∩ B| = |A| + |B| - |A ∪ B|,</span>
    <span class="keyword">def</span> <span class="function">intersect</span>(<span class="key">other</span>: HyperLogLog): <span class="predefined-type">Long</span>
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Here is roughly API that is provided by <code>HyperLogLog</code>. You can add new cookieId to it, get cardinality estimation of unique cookies that were 
already added to it, merge it with another <code>HyperLogLog</code>, and finally get intersection. It’s important to notice that
after <code>intersect</code> operation <code>HyperLogLog</code> object is lost, and you have only approximated intersection cardinality. 
So usually <code>HyperLogLog</code> intersection is the last step in computation.</p>

<p>I suggest you to watch awesome talk by <a href="https://twitter.com/avibryant">Avi Bryant</a> where he discusses not only HyperLogLog but lot’s of other
approximation data structures that can be useful for big-data analytics: <a href="http://www.infoq.com/presentations/abstract-algebra-analytics">http://www.infoq.com/presentations/abstract-algebra-analytics</a>.</p>

<h2 id="from-cookies-to-hyperloglog">From cookies to HyperLogLog</h2>

<p>We split out original impression log into two tables. </p>

<p>For ad impressions table we remove segment information and aggregate cookies, impressions and clicks by Ad and Site. <code>HyperLogLog</code> can 
be used in aggregation function exactly the same was as <code>sum</code> operation. Zero is empty <code>HyperLogLog</code>, and plus operation is <code>merge</code> (btw it’s exactly
properties required by <code>Monoid</code>)</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Ad            | Site          | Cookies HLL        | Impressions | Clicks 
————- | ————- | —————— | ———– | —— 
bmw_X5        | forbes.com    | HyperLogLog@23sdg4 | 5468        | 35   &lt;br /&gt;
bmw_X5        | cnn.com       | HyperLogLog@84jdg4 | 8943        | 29   &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<p>For segments table we remove ad and site information, and aggregate data by segment.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Segment       | Cookies HLL        | Impressions | Clicks
————- | —————— | ———– | ——
Male          | HyperLogLog@85sdg4 | 235468      | 335 &lt;br /&gt;
$100k-$150k   | HyperLogLog@35jdg4 | 569473      | 194 &lt;br /&gt;
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="percent-of-college-and-high-school-education-in-bmw-campaign">Percent of college and high school education in BMW campaign</h3>

<p>If you imaging that we can load these tables into <code>Seq</code>, then audience intersection becomes really straightforward task, that can
be solved in couple line of functional scala operations.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Audience</span>(<span class="key">ad</span>: <span class="predefined-type">String</span>, <span class="key">site</span>: <span class="predefined-type">String</span>, <span class="key">hll</span>: HyperLogLog, <span class="key">imp</span>: <span class="predefined-type">Long</span>, <span class="key">clk</span>: <span class="predefined-type">Long</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">Segment</span>(<span class="key">name</span>: <span class="predefined-type">String</span>, <span class="key">hll</span>: HyperLogLog, <span class="key">imp</span>: <span class="predefined-type">Long</span>, <span class="key">clk</span>: <span class="predefined-type">Long</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">adImpressions</span>: Seq[Audience] = <span class="error">…</span>
val <span class="key">segmentImpressions</span>: Seq[<span class="predefined-type">Segment</span>] = <span class="error">…</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">bmwCookies</span>: HyperLogLog = adImpressions
    .filter(&lt;em&gt;.ad = <span class="error">“</span>bmw_X5<span class="error">”</span>)
    .map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.hll).reduce(_ merge _)&lt;</span><span class="delimiter">/</span></span>p&gt;

&lt;p&gt;val <span class="key">educatedCookies</span>: HyperLogLog = segmentImpressions
    .filter(&lt;em&gt;.segment <span class="keyword">in</span> Seq(<span class="error">“</span>College<span class="error">”</span>, <span class="error">“</span>High School<span class="error">”</span>))
    .map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.hll).reduce( _ merge _)&lt;</span><span class="delimiter">/</span></span>p&gt;

&lt;p&gt;val p = (bmwCookies intersect educatedCookies) / bmwCookies.count()
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="spark-dataframes-with-hyperloglog">Spark DataFrames with HyperLogLog</h2>

<p>Obviously we can’t load all the data into scala <code>Seq</code> on single machine, because it’s huge, even after removing cookie level data
and transforming it into <code>HyperLogLog</code> objects, it’s around 1-2 gigabytes of data for single day.</p>

<p>So we have to use some distributed data processing framework to solve this problem, and we chose Spark.</p>

<h3 id="what-is-spark-dataframe">What is Spark DataFrame</h3>

<ul>
  <li>Inspired by R data.frame and Python/Pandas DataFrame</li>
  <li>Distributed collection of rows organized into named columns</li>
  <li>Used to be SchemaRDD in Spark &lt; 1.3.0</li>
</ul>

<h3 id="high-level-dataframe-operations">High-Level DataFrame Operations</h3>

<ul>
  <li>Selecting required columns</li>
  <li>Filtering</li>
  <li>Joining different data sets</li>
  <li>Aggregation (count, sum, average, etc)</li>
</ul>

<p>You can start from <a href="https://spark.apache.org/docs/1.3.0/sql-programming-guide.html">Spark DataFrame guide</a> or <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">DataBricks blog post</a>.</p>

<h3 id="ad-impressions-and-segments-in-dataframes">Ad impressions and segments in DataFrames</h3>

<p>We store all out data on HDFS using Parquet data format, and that’s how it looks after it’s loaded into Spark DataFrame.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val <span class="key">adImpressions</span>: DataFrame = sqlContext.parquetFile(<span class="error">“</span><span class="regexp"><span class="delimiter">/</span><span class="content">aa</span><span class="delimiter">/</span></span>audience<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;adImpressions.printSchema()
<span class="comment">// root</span>
<span class="comment">//   | – ad: string (nullable = true)</span>
<span class="comment">//   | – site: string (nullable = true)</span>
<span class="comment">//   | – hll: binary (nullable = true)</span>
<span class="comment">//   | – impressions: long (nullable = true)</span>
<span class="comment">//   | – clicks: long (nullable = true)&lt;/p&gt;</span>

&lt;p&gt;val <span class="key">segmentImpressions</span>: DataFrame = sqlContext.parquetFile(<span class="error">“</span><span class="regexp"><span class="delimiter">/</span><span class="content">aa</span><span class="delimiter">/</span></span>segments<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;segmentImpressions.printSchema()
<span class="comment">// root</span>
<span class="comment">//   | – segment: string (nullable = true)</span>
<span class="comment">//   | – hll: binary (nullable = true)</span>
<span class="comment">//   | – impressions: long (nullable = true)</span>
<span class="comment">//   | – clicks: long (nullable = true)</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p><code>HyperLogLog</code> is essentially huge <code>Array[Byte]</code> with some clever hashing and math, so it’s straightforward to store it on HDFS in serialized form.</p>

<h2 id="working-with-spark-dataframe">Working with Spark DataFrame</h2>

<p>We want to know answer for the same question: “Percent of college and high school education in BMW campaign”.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">import</span> <span class="include">org.apache.spark.sql.functions._</span>
<span class="keyword">import</span> <span class="include">org.apache.spark.sql.HLLFunctions._</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">bmwCookies</span>: HyperLogLog = adImpressions
    .filter(col(<span class="error">“</span>ad<span class="error">”</span>) === <span class="error">“</span>bmw_X5<span class="error">”</span>)
    .select(mergeHll(col(<span class="error">“</span>hll<span class="error">”</span>)).first() <span class="comment">// – sum(clicks)&lt;/p&gt;</span>

&lt;p&gt;val <span class="key">educatedCookies</span>: HyperLogLog = hllSegments
    .filter(col(<span class="error">“</span>segment<span class="error">”</span>) <span class="keyword">in</span> Seq(<span class="error">“</span>College<span class="error">”</span>, <span class="error">“</span>High School<span class="error">”</span>))
    .select(mergeHll(col(<span class="error">“</span>hll<span class="error">”</span>)).first()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val p = (bmwCookies intersect educatedCookies) / bmwCookies.count()
</pre></div>
</div>
 </figure></notextile></div></p>

<p>It looks pretty familiar, not too far from example based on scala <code>Seq</code>. Only one unusual operation, that you might notice if you have some
experience with Spark is <code>mergeHLL</code>. It’s not available in Spark by default, it’s custom <code>PartialAggregate</code> function that can compute aggregates
for serialized <code>HyperLogLog</code> objects.</p>

<h3 id="writing-your-own-spark-aggregation-function">Writing your own Spark aggregation function</h3>

<p>To write you own aggregation function you need to define function that will be applied to each row in <code>RDD</code> partition, in this example
it’s called <code>MergeHLLPartition</code>. Then you need to define function that will take results from different partitions and merge them together, for <code>HyperLogLog</code>
it’s called <code>MergeHLLMerge</code>. And finally you need to tell Spark how you want it to split your computation across <code>RDD</code> (DataFrame is backed by <code>RDD[Row]</code>) </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLLPartition</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> AggregateExpression with trees.UnaryNode[<span class="predefined-type">Expression</span>] { <span class="error">…</span> }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLLMerge</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> AggregateExpression with trees.UnaryNode[<span class="predefined-type">Expression</span>] { <span class="error">…</span> }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">MergeHLL</span>(<span class="key">child</span>: <span class="predefined-type">Expression</span>)
  <span class="directive">extends</span> PartialAggregate with trees.UnaryNode[<span class="predefined-type">Expression</span>] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override <span class="keyword">def</span> <span class="key">asPartial</span>: SplitEvaluation = {
    val partial = Alias(MergeHLLPartition(child), <span class="error">“</span>PartialMergeHLL<span class="error">”</span>)()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;SplitEvaluation(
  MergeHLLMerge(partial.toAttribute),
  partial :: Nil
)   } }
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;<span class="keyword">def</span> <span class="function">mergeHLL</span>(<span class="key">e</span>: Column): Column = MergeHLL(e.expr)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>After that writing aggregations becomes really easy task, and your expressions will look like “native” DataFrame code, which is really nice, and super
easy to read and reason about. </p>

<p>Also it works much faster then solving this problem with scala transformations on top of <code>RDD[Row]</code>, as Spark catalyst optimizer can executed optimized
plan and reduce amount of data that needs to be shuffled between spark nodes.</p>

<p>And finally it’s so much easier to manage mutable state. Spark encourage you to use immutable transformations, and it’s really cool until you need
extreme performance from your code. For example if you are using something like <code>reduce</code> or <code>aggregateByKey</code> you don’t really know when and where
your function instantiated and when it’s done with <code>RDD</code> partition and result transferred to another Spark node for merge operation. With <code>AggregateExpression</code> 
you have explicit control over mutable state, and it’s totally safe to accumulate mutable state during execution for single partition, and at the end when
you’ll need to send data to other node you can create immutable copy.</p>

<p>In this particular case using mutable <code>HyperLogLog</code> merge implementation helped to speed up computation time almost 10x times. For each partition <code>HyperLogLog</code> state
accumulated in single mutable <code>Array[Byte]</code> and at the end when data needs to be transferred somewhere else for merging with another partition, immutable copy is created.</p>

<h3 id="some-fancy-aggregates-with-dataframe-api">Some fancy aggregates with DataFrame Api</h3>

<p>You can write much more complicated aggregation functions, for example to compute aggregate based on multiple columns. Here is code sample from 
our audience analytics project.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">SegmentEstimate</span>(<span class="key">cookieHLL</span>: HyperLogLog, <span class="key">clickHLL</span>: HyperLogLog)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;type SegmentName = <span class="predefined-type">String</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">dailyEstimates</span>: RDD[(SegmentName, <span class="predefined-type">Map</span>[LocalDate, SegmentEstimate])] =
    segments.groupBy(segment_name).agg(
      segment_name,
      mergeDailySegmentEstimates(
        mkDailySegmentEstimate(      <span class="comment">// – Map[LocalDate, SegmentEstimate]</span>
          dt,
          mkSegmentEstimate(         <span class="comment">// – SegmentEstimate(cookieHLL, clickHLL)</span>
            cookie_hll,
            click_hll)
        )
      )
    )
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This codes computes daily audience aggregated by segment. Using Spark <code>PartialAggregate</code> function 
saves a lot of network traffic and minimizes distributed shuffle size. </p>

<p>This aggregation is possible because of nice properties of <code>Monoid</code></p>

<ul>
  <li><code>HyperLogLog</code> is a <code>Monoid</code> (has <code>zero</code> and <code>plus</code> operations)</li>
  <li><code>SegmentEstimate</code> is a <code>Monoid</code> (tuple of two monoids)</li>
  <li><code>Map[K, SegmentEstimate]</code> is a <code>Monoid</code> (map with value monoid value type is monoid itself)</li>
</ul>

<h3 id="problems-with-custom-aggregation-functions">Problems with custom aggregation functions</h3>

<ul>
  <li>Right now it’s closed API, so you need to place all your code under <code>org.apache.spark.sql</code> package.</li>
  <li>It’s no guarantee that it will work in next Spark release.</li>
  <li>If you want to try, I suggest you to start with <code>org.apache.spark.sql.catalyst.expressions.Sum</code> as example.</li>
</ul>

<h2 id="spark-as-in-memory-sql-database">Spark as in-memory SQL database</h2>

<p>We use Spark as in-memory database that serves SQL (composed with DataFrame Api) queries. </p>

<p>People tend to think about spark with very batch oriented mindset. Start Spark cluster in Yarn, do computation, kill cluster. Submit you application to 
standalone Spark cluster (Mesos), kill it. Biggest problem with this approach that after your application is done, and JVM is killed, <code>SparkContext</code> is lost,
and even if you are running Spark in standalone mode, all data cached by your application is lost.</p>

<p>We use Spark in totally different way. We start Spark cluster in Yarn, load data to it from HDFS, cache it in memory, and <strong>do not shutdown</strong>. We
keep JVM running, it holds a reference to <code>SparkContext</code> and keeps all the data in memory on worker nodes.</p>

<p>Our backend application is essentially very simpre REST/JSON server built with Spray, that holds <code>SparkContext</code> reference, receive requests via
URL parameters, runs queries in Spark and return response in JSON.</p>

<p>Right now (July 2015) we have data starting from April, and it’s around 100g cached in 40 nodes. We need to keep 1 year history, so we don’t expect
more than 500g. And we are very confident that we can scale horizontally without seriously affecting performance. Right now average 
request response time is 1-2 seconds which is really good for our use case.</p>

<h2 id="spark-best-practices">Spark Best practices</h2>

<p>Here are configuration options that I found really useful for our specific task. You can find more details about each of them in Spark guide.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
- spark.scheduler.mode=FAIR
- spark.yarn.executor.memoryOverhead=4000
- spark.sql.autoBroadcastJoinThreshold=300000000 // ~300mb
- spark.serializer=org.apache.spark.serializer.KryoSerializer
- spark.speculation=true
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Also I found that it’s really important to repartition your dataset if you are going to cache it and use for queries. Optimal number of partitions is
around 4-6 for each executor core, with 40 nodes and 6 executor cores we use 1000 partitions for best performance.</p>

<p>If you have too many partitions Spark will spend too much time for coordination, and receiving results from all partitions. If too small, you might have
problems with too big block during shuffle that can kill not only performance, but all your cluster: <a href="https://issues.apache.org/jira/browse/SPARK-1476">SPARK-1476</a></p>

<h2 id="other-options">Other Options</h2>

<p>Before starting this project we were evaluating some other options</p>

<h3 id="hive">Hive</h3>

<p>Obviously it’s too slow for interactive UI backend, but we found it really useful for batch data processing. We use it to process raw logs
and build aggregated tables with <code>HyperLogLog</code> inside.</p>

<h3 id="impala">Impala</h3>

<p>To get good performance out of Impala it’s required to write C++ user defined functions, and it’s was not the task that I wanted to do. Also 
I’m not confident that even with custom C++ function Impala can show performance that we need.</p>

<h4 id="druid">Druid</h4>

<p><a href="http://druid.io/">Druid</a> is really interesting project, and it’s used in another project at Collective for slightly different problem, 
but it’s not in production yet.</p>

<ul>
  <li>Managing separate Druid cluster - it’s not the task that I want to do</li>
  <li>We have batch oriented process - and druid data ingestion is stream based</li>
  <li>Bad support for some of type of queries that we need - if I need to know intersection of some particular ad with all segments, in case of druid it will be 10k (number of segments) queries, and it will obviously fail to complete in 1-2 seconds </li>
  <li>Not clear how get data back from Druid - it’s hard to get data back from Druid later, if it will turn out that it doesn’t solve out problems well</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Spark is Awesome! I didn’t have any major issues with it, and it just works! New DataFrame API is amazing, and we are going to build lot’s of new cool projects at Collective
with Spar MLLib and GraphX, and I’m pretty sure they all will be successful.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Feature Engineering at Scale With Spark]]></title>
    <link href="http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/"/>
    <updated>2015-06-10T23:02:45-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale</id>
    <content type="html"><![CDATA[<blockquote>
  <p>Check Model Matrix <a href="http://collectivemedia.github.io/modelmatrix/">Website</a> and <a href="https://github.com/collectivemedia/modelmatrix">Github project</a>.</p>
</blockquote>

<p>At <a href="http://collective.com">Collective</a> we are in programmatic advertisement business, it means that all our
advertisement decisions (what ad to show, to whom and at what time) are driven by models. We do a lot of 
machine learning, build thousands predictive models and use them to make millions decision per second.</p>

<h4 id="how-do-we-get-the-most-out-of-our-data-for-predictive-modeling">How do we get the most out of our data for predictive modeling?</h4>

<p>Success of all Machine Learning algorithms depends on data that you put into it, the better the features you choose, the
better the results you will achieve.</p>

<blockquote>
  <p>Feature Engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work better.</p>
</blockquote>

<p>In Ad-Tech it’s finite pieces of information about users that we can put into our models, and it’s 
almost the same across all companies in industry, we don’t have access to any anonymous data
like real name and age, interests on Facebook etc. It really matter how creative you are to get maximum from the data you have,
and how fast you can iterate and test new idea.</p>

<p>In 2014 Collective data science team published <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper that
describes our approach and trade-offs for audience optimization. In 2015 we solve the same problems, but
using new technologies (Spark and Spark MLLib) at even bigger scale. I want to show the tool that I built specifically 
to handle feature engineering/selection problem, and which is open sources now.</p>

<h2 id="model-matrix">Model Matrix</h2>

<!-- more -->

<h3 id="feature-transformation">Feature Transformation</h3>

<p>Imagine impression log that is used to train predictive model</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
visitor_id  | ad_campaign     | ad_id | ad_ctr     | pub_site            | state | city         | price | timestamp   &lt;br /&gt;
———– | ————— | —– | ———- | ——————- | —– | ———— | —– | ————- 
bob         | Nike_Sport      | 1     | 0.01       | http://bbc.com      | NY    | New York     | 0.17  | 1431032702135&lt;br /&gt;
bill        | Burgers_Co      | 2     | 0.005      | http://cnn.com      | CA    | Los Angeles  | 0.42  | 1431032705167 
mary        | Macys           | 3     | 0.015      | http://fashion.com  | CA    | Los Angeles  | 0.19  | 1431032708384 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Producing a feature vector for every visitor (cookie) row and every piece of information about a 
visitor as an p-size vector, where p is the number of predictor variables multiplied by cardinality 
of each variable (number of states in US, number of unique websites, etc …). It is impractical 
both from the data processing standpoint and because the resulting vector would only have 
about 1 in 100,000 non-zero elements.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
 visitor_id  | Nike_Sport | Burgers_Co | Macys | NY  | CA  | … 
 ———– | ———- | ———- | —– | — | — | — 
 bob         | 1.0        |            |       | 1.0 |     | … 
 bill        |            | 1.0        |       |     | 1.0 | … 
 mary        |            |            | 1.0   |     | 1.0 | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Model Matrix uses feature transformations (top, index, binning) to reduce dimensionality to arrive 
at between one and two thousand predictor variables, with data sparsity of about 1 in 10. It removes 
irrelevant and low frequency predictor values from the model, and transforms continuous 
variable into bins of the same size.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre> &lt;br /&gt;
 visitor_id  | Nike | OtherAd | NY  | OtherState | price ∈ [0.01, 0.20) | price ∈ [0.20, 0.90) | … 
 ———– | —- | ——- | — | ———- | ——————– | ——————– | — 
 bob         | 1.0  |         | 1.0 |            | 1.0                  |                      | … 
 bill        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
 mary        |      | 1.0     |     | 1.0        |                      | 1.0                  | … 
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Transformation definitions in scala:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Absence of transformation
 */</span>
<span class="keyword">case</span> object <span class="predefined-type">Identity</span> <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find top values
 * by a quantity that cumulatively cover a given percentage
 * of this quantity. For example, find the top DMAs that
 * represent 99% of cookies, or find top sites that
 * are responsible for 90% of impressions.
 *
 * @param cover      cumulative cover percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Top</span>(<span class="key">cover</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * For distinct values of the column, find the values
 * with at least the minimum support in the data set.
 * Support for a value is defined as the percentage of a
 * total quantity that have that value. For example,
 * find segments that appear for at least 1% of the cookies.
 *
 * @param support    support percentage
 * @param allOther   include feature for all other values
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Index</span>(<span class="key">support</span>: <span class="predefined-type">Double</span>, <span class="key">allOther</span>: <span class="predefined-type">Boolean</span>) <span class="directive">extends</span> Transform&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
 * Break the values in the column into bins with roughly the same number of points.
 *
 * @param nbins target number of bins
 * @param minPoints minimum number of points in single bin
 * @param minPercents minimum percent of points in a bin (0-100).
 *                    The larger of absolute number and percent points is used.
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Bins</span>(<span class="key">nbins</span>: Int, <span class="key">minPoints</span>: Int = <span class="integer">0</span>, <span class="key">minPercents</span>: <span class="predefined-type">Double</span> = <span class="float">0.0</span>) <span class="directive">extends</span> Transform
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="transformed-columns">Transformed Columns</h3>

<h4 id="categorical-transformation">Categorical Transformation</h4>

<p>A column calculated by applying top or index transform function, each columns id corresponds 
to one unique value from input data set. SourceValue is encoded as ByteVector unique value from 
input column and used later for featurization. </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">CategoricalTransformer</span>(
  <span class="key">features</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(features) {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[CategoricalColumn]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait CategoricalColumn {
  <span class="keyword">def</span> <span class="key">columnId</span>: Int
  <span class="keyword">def</span> <span class="key">count</span>: <span class="predefined-type">Long</span>
  <span class="keyword">def</span> <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object CategoricalColumn {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">CategoricalValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">sourceName</span>: <span class="predefined-type">String</span>,
    <span class="key">sourceValue</span>: ByteVector,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">AllOther</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">cumulativeCount</span>: <span class="predefined-type">Long</span>
  ) <span class="directive">extends</span> CategoricalColumn &lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="bin-column">Bin Column</h4>

<p>A column calculated by applying binning transform function.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">BinsTransformer</span>(
  <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> <span class="predefined-type">Transformer</span>.Features
) <span class="directive">extends</span> <span class="predefined-type">Transformer</span>(input) with Binner {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">transform</span>(<span class="key">feature</span>: TypedModelFeature): Seq[BinColumn] = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">BinValue</span>(
    <span class="key">columnId</span>: Int,
    <span class="key">low</span>: <span class="predefined-type">Double</span>,
    <span class="key">high</span>: <span class="predefined-type">Double</span>,
    <span class="key">count</span>: <span class="predefined-type">Long</span>,
    <span class="key">sampleSize</span>: <span class="predefined-type">Long</span>
  ) 
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="building-model-matrix-instance">Building Model Matrix Instance</h3>

<p>Model Matrix instance contains information about shape of the training data, what transformations (categorical and binning)
are required to apply to input data in order to obtain feature vector that will got into machine learning
algorithm.</p>

<p>Building model matrix instance described well in <a href="http://collectivemedia.github.io/modelmatrix/doc/cli.html">command line interface documentation</a>.</p>

<h3 id="featurizing-your-data">Featurizing your data</h3>

<p>When you have model matrix instance, you can apply it to multiple input data sets. For example in Collective
we build model matrix instance once a week or even month, and use it for building models from daily/hourly data.
It gives us nice property: all models have the same columns, and it’s easy to compare them.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Similar to Spark LabeledPoint</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">IdentifiedPoint</span>(<span class="key">id</span>: Any, <span class="key">features</span>: <span class="predefined-type">Vector</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="type">class</span> <span class="class">Featurization</span>(<span class="key">features</span>: Seq[ModelInstanceFeature]) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Check that all input features belong to the same model instance</span>
  <span class="directive">private</span> val instances = features.map(_.modelInstanceId).toSet
  require(instances.size == <span class="integer">1</span>, 
    s<span class="error">”</span>Features belong to different model <span class="key">instances</span>: <span class="error">$</span>instances<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Maximum columns id in instance features</span>
  <span class="directive">private</span> val totalNumberOfColumns = features.flatMap {
    <span class="keyword">case</span> ModelInstanceIdentityFeature(&lt;em&gt;, _, _, _, columnId) =&amp;gt; Seq(columnId)
    <span class="keyword">case</span> ModelInstanceTopFeature(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;, &lt;em&gt;, _, _, cols) =&amp;gt; cols.map(&lt;</span><span class="delimiter">/</span></span>em&gt;.columnId)
    <span class="keyword">case</span> ModelInstanceIndexFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>    <span class="keyword">case</span> ModelInstanceBinsFeature(&lt;em&gt;, _, _, _, cols) =&amp;gt; cols.map(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.columnId)</span></span><span class="error">
</span>  }.max&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">/**
   * Featurize input dataset
   *
   * @return id data type and featurized rows
   */</span>
  <span class="keyword">def</span> <span class="function">featurize</span>(
    <span class="key">input</span>: DataFrame <span class="error">@</span><span class="error">@</span> FeaturesWithId, 
    <span class="key">idColumn</span>: <span class="predefined-type">String</span>
  ): (DataType, RDD[IdentifiedPoint]) = {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Extract features from input DataFrame with id column: </span><span class="inline"><span class="inline-delimiter">$</span>idColumn</span><span class="content">. </span><span class="delimiter">&quot;</span></span> + 
         s<span class="string"><span class="delimiter">&quot;</span><span class="content">Total number of columns: </span><span class="inline"><span class="inline-delimiter">$</span>totalNumberOfColumns</span><span class="delimiter">&quot;</span></span>)

...
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;}
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="results">Results</h3>

<p>Model Matrix is open sourced, and available on <a href="https://github.com/collectivemedia/modelmatrix">Github</a>, lot’s of 
documentation on <a href="http://collectivemedia.github.io/modelmatrix/">Website</a>.</p>

<p>We use it at <a href="http://collective.com">Collective</a> to define our models and it works for us really well.</p>

<p>You can continue your reading with <a href="http://arxiv.org/abs/1402.6076">Machine Learning at Scale</a> paper, 
to get more data science focused details about our modeling approach.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Twitter Live Stream Analytics With Spark and Cassandra]]></title>
    <link href="http://eugenezhulenev.com/blog/2014/11/20/twitter-analytics-with-spark/"/>
    <updated>2014-11-20T20:01:15-05:00</updated>
    <id>http://eugenezhulenev.com/blog/2014/11/20/twitter-analytics-with-spark</id>
    <content type="html"><![CDATA[<blockquote>
  <p>This is repost of my article from <a href="http://io.pellucid.com/blog/building-twitter-live-stream-analytics-with-spark-and-cassandra">Pellucid Tech Blog</a></p>
</blockquote>

<h3 id="background">Background</h3>

<p>At <a href="http://pellucid.com">Pellucid Analytics</a> we we are building a platform that
automates and simplifies the creation of data-driven chartbooks, so that it takes
minutes instead of hours to get from raw data to powerful visualizations and compelling stories.</p>

<p>One of industries we are focusing on is Investment Banking. We are helping IB advisory
professionals build pitch-books, and provide them with analytical and quantitative support
to sell their ideas. Comparable Companies Analysis is central to this business.</p>

<blockquote>
  <p>Comparable company analysis starts with establishing a peer group consisting of similar companies of similar size in the same industry and region.</p>
</blockquote>

<p>The problem we are faced with is finding a scalable solution to establish a peer group for any chosen company.</p>

<!-- more -->

<h3 id="approaches-that-we-tried">Approaches That We Tried</h3>

<h4 id="company-industry">Company Industry</h4>

<p>Data vendors provide <a href="http://en.wikipedia.org/wiki/Industry_classification">industry classification</a>
for each company, and it helps a lot in industries like retail (Wal-Mart is good comparable to Costco),
energy (Chevron and Exxon Mobil) but it stumbles with many other companies. People tend to compare
Amazon with Google as a two major players in it business, but data vendors tend to put Amazon in retail industry with Wal-Mart/Costco as comparables.</p>

<h4 id="company-financials-and-valuation-multiples">Company Financials and Valuation Multiples</h4>

<p>We tried cluster analysis and k-nearest neighbors to group companies based on their
financials (Sales, Revenue) and valuation multiples (EV/EBIDTA, P/E). However assumptions
that similar companies will have similar valuations multiples is wrong. People compare
Twitter with Facebook as two biggest companies in social media, but based on their financials
they don’t have too much in common. Facebook 2013 revenue is almost $8 billion and Twitter has only $600 million.</p>

<h3 id="novel-approach">Novel Approach</h3>

<p>We came up with an idea that if companies are often mentioned in news articles and tweets together, it’s probably a sign that people think about them as comparable companies. In this post I’ll show how we built proof of concept for this idea with Spark, Spark Streaming and Cassandra. We use only Twitter live stream data for now, accessing high quality news data is a bit more complicated problem.</p>

<!-- more -->

<p>Let’s take for example this tweet from CNN:</p>

<blockquote class="twitter-tweet" lang="en"><p>Trying to spot the next <a href="https://twitter.com/search?q=%24FB&amp;src=ctag">$FB</a> or <a href="https://twitter.com/search?q=%24TWTR&amp;src=ctag">$TWTR</a>? These 10 startups are worth keeping an eye on <a href="http://t.co/FEKNtm7QqB">http://t.co/FEKNtm7QqB</a></p>&mdash; CNN Public Relations (@CNNPR) <a href="https://twitter.com/CNNPR/status/518083527863435264">October 3, 2014</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>From this tweet we can derive 2 mentions for 2 companies. For Facebook it will be Twitter and vice-versa. If we collect tweets for all companies over some period of time, and take a ratio of joint appearance in same tweet as a measure of “similarity”, we can build comparable company recommendations based on this measure.</p>

<h3 id="data-model">Data Model</h3>

<p>We use <a href="http://cassandra.apache.org/">Cassandra</a> to store all mentions, aggregates and final recommendations.
We use <a href="https://github.com/websudos/phantom">Phantom DSL</a> for scala to define schema
and for most of Cassandra operations (spark integration is not yet supported in Phantom).</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">/**
 * Mention of focus company
 *
 * @param ticker   ticker of focus company
 * @param source   source of this mention (Twitter, RSS, etc…)
 * @param sourceId source specific id
 * @param time     time
 * @param mentions set of other tickers including focus ticker itself
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Mention</span>(<span class="key">ticker</span>: Ticker, <span class="key">source</span>: <span class="predefined-type">String</span>, <span class="key">sourceId</span>: <span class="predefined-type">String</span>, <span class="key">time</span>: DateTime, <span class="key">mentions</span>: <span class="predefined-type">Set</span>[Ticker])&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;sealed <span class="type">class</span> <span class="class">MentionRecord</span> <span class="directive">extends</span> CassandraTable[MentionRecord, Mention] with <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override val <span class="key">tableName</span>: <span class="predefined-type">String</span> = <span class="error">“</span>mention<span class="error">”</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object ticker    <span class="directive">extends</span> StringColumn    (<span class="local-variable">this</span>)  with PartitionKey[<span class="predefined-type">String</span>]
  object source    <span class="directive">extends</span> StringColumn    (<span class="local-variable">this</span>)  with PrimaryKey[<span class="predefined-type">String</span>]
  object time      <span class="directive">extends</span> DateTimeColumn  (<span class="local-variable">this</span>)  with PrimaryKey[DateTime]
  object source_id <span class="directive">extends</span> StringColumn    (<span class="local-variable">this</span>)  with PrimaryKey[<span class="predefined-type">String</span>]
  object mentions  <span class="directive">extends</span> SetColumn[MentionRecord, Mention, <span class="predefined-type">String</span>] (<span class="local-variable">this</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">fromRow</span>(<span class="key">r</span>: Row): Mention = {
    Mention(Ticker(ticker(r)), source(r), source_id(r), time(r), mentions(r) map Ticker)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">/**
 * Count mentions for each ticker pair
 *
 * @param ticker        ticker of focus company
 * @param mentionedWith mentioned with this ticker
 * @param count         number of mentions
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">MentionsAggregate</span>(<span class="key">ticker</span>: Ticker, <span class="key">mentionedWith</span>: Ticker, <span class="key">count</span>: <span class="predefined-type">Long</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;sealed <span class="type">class</span> <span class="class">MentionsAggregateRecord</span> <span class="directive">extends</span> CassandraTable[MentionsAggregateRecord, MentionsAggregate] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override val <span class="key">tableName</span>: <span class="predefined-type">String</span> = <span class="error">“</span>mentions_aggregate<span class="error">”</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object ticker         <span class="directive">extends</span> StringColumn (<span class="local-variable">this</span>) with PartitionKey[<span class="predefined-type">String</span>]
  object mentioned_with <span class="directive">extends</span> StringColumn (<span class="local-variable">this</span>) with PrimaryKey[<span class="predefined-type">String</span>]
  object counter        <span class="directive">extends</span> LongColumn   (<span class="local-variable">this</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">fromRow</span>(<span class="key">r</span>: Row): MentionsAggregate = {
    MentionsAggregate(Ticker(ticker(r)), Ticker(mentioned_with(r)), counter(r))
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">/**
 * Recommendation built based on company mentions with other companies
 *
 * @param ticker         focus company ticker
 * @position             recommendation position
 * @param recommendation recommended company ticker
 * @param p              number of times recommended company mentioned together
 *                       with focus company divided by total focus company mentions
 */</span>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">Recommendation</span>(<span class="key">ticker</span>: Ticker, <span class="key">position</span>: <span class="predefined-type">Long</span>, <span class="key">recommendation</span>: Ticker, <span class="key">p</span>: <span class="predefined-type">Double</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;sealed <span class="type">class</span> <span class="class">RecommendationRecord</span> <span class="directive">extends</span> CassandraTable[RecommendationRecord, Recommendation] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override val <span class="key">tableName</span>: <span class="predefined-type">String</span> = <span class="error">“</span>recommendation<span class="error">”</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object ticker         <span class="directive">extends</span> StringColumn (<span class="local-variable">this</span>) with PartitionKey[<span class="predefined-type">String</span>]
  object position       <span class="directive">extends</span> LongColumn   (<span class="local-variable">this</span>) with PrimaryKey[<span class="predefined-type">Long</span>]
  object recommendation <span class="directive">extends</span> StringColumn (<span class="local-variable">this</span>)
  object p              <span class="directive">extends</span> DoubleColumn (<span class="local-variable">this</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">fromRow</span>(<span class="key">r</span>: Row): Recommendation = {
    Recommendation(Ticker(ticker(r)), position(r), Ticker(recommendation(r)), p(r))
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="ingest-real-time-twitter-stream">Ingest Real-Time Twitter Stream</h3>

<p>We use <a href="https://spark.apache.org/streaming/">Spark Streaming</a> Twitter integration to subscribe for
real-time twitter updates, then we extract company mentions and put them to Cassandra. Unfortunately Phantom
doesn’t support Spark yet, so we used <a href="https://github.com/datastax/spark-cassandra-connector">Datastax Spark Cassandra Connector</a>
with custom type mappers to map from Phantom-record types into Cassandra tables.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">MentionStreamFunctions</span>(<span class="annotation">@transient</span> <span class="key">stream</span>: DStream[Mention]) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">import</span> <span class="include">TickerTypeConverter._</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;TypeConverter.registerConverter(StringToTickerTypeConverter)
  TypeConverter.registerConverter(TickerToStringTypeConverter)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;implicit object MentionMapper <span class="directive">extends</span> DefaultColumnMapper&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">Map(</span><span class="delimiter">&quot;</span></span> title=<span class="string"><span class="delimiter">&quot;</span><span class="content">ticker&amp;quot;        -&amp;gt; &amp;quot;ticker&amp;quot;,</span></span><span class="error">
</span>    &amp;quot;source&amp;quot;        -&amp;gt; &amp;quot;source&amp;quot;,
    &amp;quot;sourceId&amp;quot;      -&amp;gt; &amp;quot;source_id&amp;quot;,
    &amp;quot;time&amp;quot;          -&amp;gt; &amp;quot;time&amp;quot;,
    &amp;quot;mentions&amp;quot;      -&amp;gt; &amp;quot;mentions<span class="string"><span class="delimiter">&quot;</span><span class="content">&gt;Mention&lt;/a&gt;)&lt;/p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">saveMentionsToCassandra</span>(<span class="key">keyspace</span>: <span class="predefined-type">String</span>) = {
    stream.saveToCassandra(keyspace, MentionRecord.tableName)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
  <span class="directive">private</span> val filters = Companies.load().map(c =&amp;gt; s<span class="error">”</span><span class="error">$</span><span class="error">$</span><span class="error">$</span>{c.ticker.value}<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val sc = <span class="keyword">new</span> SparkContext(sparkConf)
  val ssc = <span class="keyword">new</span> StreamingContext(sc, Seconds(<span class="integer">2</span>))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val stream = TwitterUtils.createStream(ssc, None, filters = filters)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Save Twitter Stream to cassandra</span>
  stream.foreachRDD(updates =&amp;gt; log.info(s<span class="error">”</span>Received Twitter stream updates. Count: <span class="error">$</span>{updates.count()}<span class="error">”</span>))
  stream.extractMentions.saveMentionsToCassandra(keySpace)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Start Streaming Application</span>
  ssc.start()
</pre></div>
</div>
 </figure></notextile></div></p>

<h3 id="spark-for-aggregation-and-recommendation">Spark For Aggregation and Recommendation</h3>

<p>To come up with comparable company recommendation we use 2-step process.</p>

<h5 id="count-mentions-for-each-pair-of-tickers">1. Count mentions for each pair of tickers</h5>

<p>After <code>Mentions</code> table loaded in Spark as <code>RDD[Mention]</code> we extract pairs of tickers,
and it enables bunch of aggregate and reduce functions from Spark <code>PairRDDFunctions</code>.
With <code>aggregateByKey</code> and given combine functions we efficiently build counter map <code>Map[Ticker, Long]</code> for each
ticker distributed in cluster. From single <code>Map[Ticker, Long]</code> we emit multiple aggregates for each ticket pair.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">AggregateMentions</span>(<span class="annotation">@transient</span> <span class="key">sc</span>: SparkContext, <span class="key">keyspace</span>: <span class="predefined-type">String</span>)
  <span class="directive">extends</span> CassandraMappers with <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> type Counter = <span class="predefined-type">Map</span>[Ticker, <span class="predefined-type">Long</span>]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> implicit lazy val summ = Semigroup.instance&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">_ + _</span><span class="delimiter">&quot;</span></span>&gt;<span class="predefined-type">Long</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;&lt;</span><span class="delimiter">/</span></span>p&gt;

&lt;p&gt;<span class="directive">private</span> lazy val <span class="key">seqOp</span>: (Counter, Ticker) =&amp;gt; Counter = {
    <span class="keyword">case</span> (counter, ticker) <span class="keyword">if</span> counter.isDefinedAt(ticker) =&amp;gt; counter.updated(ticker, counter(ticker) + <span class="integer">1</span>)
    <span class="keyword">case</span> (counter, ticker) =&amp;gt; counter + (ticker -&amp;gt; <span class="integer">1</span>)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> lazy val <span class="key">combOp</span>: (Counter, Counter) =&amp;gt; Counter = {
    <span class="keyword">case</span> (l, r) =&amp;gt; implicitly[Monoid[Counter]].append(l, r)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">aggregate</span>(): Unit = {
    <span class="comment">// Emit pairs of (Focus Company Ticker, Mentioned With)</span>
    val pairs = sc.cassandraTable&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">keyspace, MentionRecord.tableName</span><span class="delimiter">&quot;</span></span>&gt;Mention&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;.</span></span><span class="error">
</span>      flatMap(mention =&amp;gt; mention.mentions.map((mention.ticker, _)))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;<span class="comment">// Calculate mentions for each ticker</span>
val aggregated = pairs.aggregateByKey(<span class="predefined-type">Map</span>.empty[Ticker, <span class="predefined-type">Long</span>])(seqOp, combOp)

<span class="comment">// Build MentionsAggregate from counters</span>
val mentionsAggregate = aggregated flatMap {
  <span class="keyword">case</span> (ticker, counter) =&amp;gt; counter map {
    <span class="keyword">case</span> (mentionedWith, count) =&amp;gt; MentionsAggregate(ticker, mentionedWith, count)
  }
}

mentionsAggregate.saveToCassandra(keyspace, MentionsAggregateRecord.tableName)   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h5 id="sort-aggregates-and-build-recommendations">2. Sort aggregates and build recommendations</h5>

<p>After aggregates computed, we sort them globally and then group them by key (Ticker). After
all aggregates grouped we produce <code>Recommendation</code> in single traverse distributed for each key.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">Recommend</span>(<span class="annotation">@transient</span> <span class="key">sc</span>: SparkContext, <span class="key">keyspace</span>: <span class="predefined-type">String</span>)
  <span class="directive">extends</span> CassandraMappers with <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> <span class="keyword">def</span> <span class="key">toRecommendation</span>: (MentionsAggregate, Int) =&amp;gt; Recommendation = {
    var <span class="key">totalMentions</span>: <span class="predefined-type">Option</span>[<span class="predefined-type">Long</span>] = None&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;{
  <span class="keyword">case</span> (aggregate, idx) <span class="keyword">if</span> totalMentions.isEmpty =&amp;gt;
    totalMentions = Some(aggregate.count)
    Recommendation(aggregate.ticker, idx, aggregate.mentionedWith, <span class="integer">1</span>)

  <span class="keyword">case</span> (aggregate, idx) =&amp;gt;
    Recommendation(aggregate.ticker, idx,
                   aggregate.mentionedWith,
                   aggregate.count.toDouble / totalMentions.get)
}   }
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;<span class="keyword">def</span> <span class="function">recommend</span>(): Unit = {
    val aggregates = sc.
               cassandraTable&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">keyspace, MentionsAggregateRecord.tableName</span><span class="delimiter">&quot;</span></span>&gt;MentionsAggregate&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;.</span></span><span class="error">
</span>               sortBy(_.count, ascending = <span class="predefined-constant">false</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val recommendations = aggregates.
  groupBy(_.ticker).
  mapValues(_.zipWithIndex).
  flatMapValues(_ map toRecommendation.tupled).values

recommendations.saveToCassandra(keyspace, RecommendationRecord.tableName)   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h3 id="results">Results</h3>

<p>You can check comparable company recommendations build from Twitter stream using <a href="http://pellucidanalytics.github.io/tweet-driven-comparable-companies/comparables/comps.html">this link</a>.</p>

<p>Cassandra and Spark works perfectly together and allows you to build scalable data-driven applications, that are super easy to scale out and handle gigabytes and terabytes of data. In this particular case, it’s probably an overkill. Twitter doesn’t have enough finance-related activity to produce serious load. However it’s easy to extend this application and add other streams: Bloomberg News Feed, Thompson Reuters, etc.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/tweet-driven-comparable-companies">Github</a></p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stock Price Prediction With Big Data and Machine Learning]]></title>
    <link href="http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning/"/>
    <updated>2014-11-14T21:03:35-05:00</updated>
    <id>http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning</id>
    <content type="html"><![CDATA[<p>Apache Spark and Spark MLLib for building price movement prediction model from order log data.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/orderbook-dynamics">Github</a></p>
</blockquote>

<h3 id="synopsis">Synopsis</h3>

<p>This post is based on <a href="https://raw.github.com/ezhulenev/scala-openbook/master/assets/Modeling-high-frequency-limit-order-book-dynamics-with-support-vector-machines.pdf">Modeling high-frequency limit order book dynamics with support vector machines</a> paper.
Roughly speaking I’m implementing ideas introduced in this paper in scala with <a href="https://spark.apache.org/">Spark</a> and <a href="https://spark.apache.org/mllib/">Spark MLLib</a>.
Authors are using sampling, I’m going to use full order log from <a href="http://www.nyxdata.com/Data-Products/NYSE-OpenBook-History">NYSE</a> (sample data is available from <a href="ftp://ftp.nyxdata.com/Historical%20Data%20Samples/TAQ%20NYSE%20OpenBook/">NYSE FTP</a>), just because
I can easily do it with Spark. Instead of using SVM, I’m going to use <a href="http://spark.apache.org/docs/latest/mllib-decision-tree.html">Decision Tree</a> algorithm for classification,
because in Spark MLLib it supports multiclass classification out of the box.</p>

<p>If you want to get deep understanding of the problem and proposed solution, you need to read the paper.
I’m going to give high level overview of the problem in less academic language, in one or two paragraphs.</p>

<blockquote>
  <p>Predictive modelling is the process by which a model is created or chosen to try to best predict the probability of an outcome.</p>
</blockquote>

<!-- more -->

<h4 id="model-architecture">Model Architecture</h4>

<p>Authors are proposing framework for extracting feature vectors from from raw order log data, that can be used as input to
machine learning classification method (SVM or Decision Tree for example) to predict price movement (Up, Down, Stationary). Given a set of training data
with assigned labels (price movement) classification algorithm builds a model that assigns new examples into one of pre-defined categories.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
Time(sec)            Price($)   Volume      Event Type      Direction
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
34203.011926972      598.68     10          submission      ask
34203.011926973      594.47     15          submission      bid
34203.011926974      594.49     20          submission      bid
34203.011926981      597.68     30          submission      ask
34203.011926991      594.47     15          execution       ask
34203.011927072      597.68     10          cancellation    ask
34203.011927082      599.88     12          submission      ask
34203.011927097      598.38     11          submission      ask
</pre></div>
</div>
 </figure></notextile></div></p>

<p>In the table, each row of the message book represents a trading event that could be either a order submission,
order cancellation, or order execution. The arrival time measured from midnight,
is in seconds and nanoseconds; price is in US dollars, and the Volume is in number of shares.
Ask - I’m selling and asking for this price, Bid - I want to buy for this price.</p>

<p>From this log it’s very easy to reconstruct state of order book after each entry. You can read more about <a href="http://www.investopedia.com/terms/o/order-book.asp">order book</a>
and <a href="http://www.investopedia.com/university/intro-to-order-types/limit-orders.asp">limit order book</a> in Investopedia,
I’m not going to dive into details. Concepts are super easy for understanding.</p>

<blockquote>
  <p>An electronic list of buy and sell orders for a specific security or financial instrument, organized by price level.</p>
</blockquote>

<h4 id="feature-extraction-and-training-data-preparation">Feature Extraction and Training Data Preparation</h4>

<p>After order books are reconstructed from order log, we can derive attributes, that will form feature vectors used as input to <code>classification model</code>.</p>

<p>Attributes are divided into three categories: basic, time-insensitive, and time-sensitive, all of which can be directly computed from the data.
Attributes in the basic set are the prices and volumes at both ask and bid sides up to n = 10 different levels (that is, price levels in the order book at a given moment),
which can be directly fetched from the order book. Attributes in the time-insensitive set are easily computed from the basic set at a single point in time.
Of this, bid-ask spread and mid-price, price ranges, as well as average price and volume at different price levels are calculated in feature sets <code>v2</code>, <code>v3</code>, and <code>v5</code>, respectively;
while <code>v5</code> is designed to track the accumulated differences of price and volume between ask and bid sides. By further taking the recent history of current data into consideration,
we devise the features in the time-sensitive set. More about calculating other attributes can be found in <a href="https://raw.github.com/ezhulenev/scala-openbook/master/assets/Modeling-high-frequency-limit-order-book-dynamics-with-support-vector-machines.pdf">original paper</a>.</p>

<p><img class="center" src="https://raw.github.com/ezhulenev/scala-openbook/master/assets/features.png"></p>

<h4 id="labeling-training-data">Labeling Training Data</h4>

<p>To prepare training data for machine learning it’s also required to label each point with price movement observed over some time horizon (1 second fo example).
It’s straightforward task that only requires two order books: current order book and order book after some period of time.</p>

<p>I’m going to use <code>MeanPriceMove</code> label that can be: <code>Stationary</code>, <code>Up</code> or <code>Down</code>.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;trait <span class="predefined-type">Label</span>[L] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { label =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">current</span>: OrderBook, <span class="key">future</span>: OrderBook): <span class="predefined-type">Option</span>[L]
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;sealed trait MeanPriceMove&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object MeanPriceMove {
  <span class="keyword">case</span> object Up <span class="directive">extends</span> MeanPriceMove
  <span class="keyword">case</span> object Down <span class="directive">extends</span> MeanPriceMove
  <span class="keyword">case</span> object Stationary <span class="directive">extends</span> MeanPriceMove
}&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object MeanPriceMovementLabel <span class="directive">extends</span> <span class="predefined-type">Label</span>[MeanPriceMove] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span>[<span class="local-variable">this</span>] val basicSet = BasicSet.apply(BasicSet.Config.default)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">apply</span>(<span class="key">current</span>: OrderBook, <span class="key">future</span>: OrderBook): <span class="predefined-type">Option</span>[MeanPriceMove] = {
    val currentMeanPrice = basicSet.meanPrice(current)
    val futureMeanPrice = basicSet.meanPrice(future)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val <span class="key">cell</span>: Cell[MeanPriceMove] =
   currentMeanPrice.zipMap(futureMeanPrice) {
    (currentMeanValue, futureMeanValue) =&amp;gt;
      <span class="keyword">if</span> (currentMeanValue == futureMeanValue)
        MeanPriceMove.Stationary
      <span class="keyword">else</span> <span class="keyword">if</span> (currentMeanValue &amp;gt; futureMeanValue)
        MeanPriceMove.Down
      <span class="keyword">else</span>
        MeanPriceMove.Up
    }

cell.toOption   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h3 id="order-log-data">Order Log Data</h3>

<p>I’m going to use <a href="http://www.nyxdata.com/Data-Products/NYSE-OpenBook-History">NYSE TAQ OpenBook</a> orders data, and parse it with <a href="https://github.com/ezhulenev/scala-openbook">Scala OpenBook</a>
library. It’s easiest data set to get, free sample data for 2 trading days is available for download at <a href="ftp://ftp.nyxdata.com/Historical%20Data%20Samples/TAQ%20NYSE%20OpenBook/">NYSE FTP</a>.</p>

<blockquote>
  <p>TAQ (Trades and Quotes) historical data products provide a varying range of market depth on a T+1 basis for covered markets.
TAQ data products are used to develop and backtest trading strategies, analyze market trends as seen in a real-time ticker plant environment, and research markets for regulatory or audit activity.</p>
</blockquote>

<h3 id="prepare-training-data">Prepare Training Data</h3>

<p><code>OrderBook</code> is two sorted maps, where key is price and value is volume.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="keyword">case</span> <span class="type">class</span> <span class="class">OrderBook</span>(<span class="key">symbol</span>: <span class="predefined-type">String</span>,
                     <span class="key">buy</span>: <span class="predefined-type">TreeMap</span>[Int, Int] = <span class="predefined-type">TreeMap</span>.empty,
                     <span class="key">sell</span>: <span class="predefined-type">TreeMap</span>[Int, Int] = <span class="predefined-type">TreeMap</span>.empty)
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="feature-sets">Feature Sets</h4>

<p>I’m using <code>Cell</code> from <a href="https://github.com/pellucidanalytics/framian">Framian</a> library to represent extracted feature values. It can be <code>Value</code>, <code>NA</code> or <code>NM</code>.</p>

<p>As defined in original paper we have three feature sets, first two calculated from <code>OrderBook</code>, last one requires <code>OrdersTrail</code> which effectively is
window computation over raw order log.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait <span class="predefined-type">BasicAttribute</span>[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: BasicAttribute[T2] = new BasicAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T2] = self(orderBook).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
sealed trait TimeInsensitiveAttribute[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: TimeInsensitiveAttribute[T2] = new TimeInsensitiveAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T2] = self(orderBook).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
trait TimeSensitiveAttribute[T] <span class="directive">extends</span> <span class="predefined-type">Serializable</span> { self =&amp;gt;
  <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">ordersTrail</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): Cell[T]&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> map&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: T =&amp;gt; T2</span><span class="delimiter">&quot;</span></span>&gt;T2&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: TimeSensitiveAttribute[T2] = new TimeSensitiveAttribute[T2] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">ordersTrail</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): Cell[T2] = self(ordersTrail).map(f)
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p>and it’s how features calculation looks like</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">BasicSet</span> <span class="directive">private</span>[attribute] (val <span class="key">config</span>: BasicSet.Config) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {
  <span class="directive">private</span>[attribute] <span class="keyword">def</span> <span class="function">askPrice</span>(<span class="key">orderBook</span>: OrderBook)(<span class="key">i</span>: Int): Cell[Int] = {
    Cell.fromOption {
      orderBook.sell.keySet.drop(i - <span class="integer">1</span>).headOption
    }
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span>[attribute] <span class="keyword">def</span> <span class="function">bidPrice</span>(<span class="key">orderBook</span>: OrderBook)(<span class="key">i</span>: Int): Cell[Int] = {
    Cell.fromOption {
      val bidPrices = orderBook.buy.keySet
      <span class="keyword">if</span> (bidPrices.size &amp;gt;= i) {
        bidPrices.drop(bidPrices.size - i).headOption
      } <span class="keyword">else</span> None
    }
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> <span class="keyword">def</span> attribute&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">f: OrderBook =&amp;gt; Cell[T]</span><span class="delimiter">&quot;</span></span>&gt;T&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;: BasicAttribute[T] = new BasicAttribute[T] {</span></span><span class="error">
</span>    <span class="keyword">def</span> <span class="function">apply</span>(<span class="key">orderBook</span>: OrderBook): Cell[T] = f(orderBook)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">askPrice</span>(<span class="key">i</span>: Int): <span class="predefined-type">BasicAttribute</span>[Int] = attribute(askPrice(_)(i))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">bidPrice</span>(<span class="key">i</span>: Int): <span class="predefined-type">BasicAttribute</span>[Int] = attribute(bidPrice(_)(i))&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val <span class="key">meanPrice</span>: <span class="predefined-type">BasicAttribute</span>[<span class="predefined-type">Double</span>] = {
    val ask1 = askPrice(<span class="integer">1</span>)
    val bid1 = bidPrice(<span class="integer">1</span>)
    <span class="predefined-type">BasicAttribute</span>.from(orderBook =&amp;gt;
      ask1(orderBook).zipMap(bid1(orderBook)) {
        (ask, bid) =&amp;gt; (ask.toDouble + bid.toDouble) / <span class="integer">2</span>
      })
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<h4 id="label-training-data">Label Training Data</h4>

<p>To extract labeled data from orders I’m using <code>LabeledPointsExtractor</code></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">LabeledPointsExtractor</span>[<span class="key">L</span>: LabelEncode] {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">labeledPoints</span>(<span class="key">orders</span>: <span class="predefined-type">Vector</span>[OpenBookMsg]): <span class="predefined-type">Vector</span>[LabeledPoint] = {
    log.debug(s<span class="error">”</span>Extract labeled points from orders log. Log <span class="key">size</span>: <span class="error">$</span>{orders.size}<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;<span class="comment">// ...   } } </span></pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<p>and it can be constructed nicely with builder</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
val extractor = {
    <span class="keyword">import</span> <span class="include">com.scalafi.dynamics.attribute.LabeledPointsExtractor._</span>
    (LabeledPointsExtractor.newBuilder()
      += basic(&lt;em&gt;.askPrice(<span class="integer">1</span>))
      += basic(&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">em&gt;.bidPrice(1))</span></span><span class="error">
</span>      += basic(_.meanPrice)
      ).result(symbol, MeanPriceMovementLabel, LabeledPointsExtractor.Config(<span class="integer">1</span>.millisecond))
  }
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This <code>extractor</code> will prepare labeled points using <code>MeanPriceMovementLabel</code> with 3 features: ask price, bid price and mean price</p>

<h3 id="run-classification-model">Run Classification Model</h3>

<p>In “real” application I’m using 36 features from all 3 feature sets. I run my tests with sample data from NYSE ftp,
<code>EQY_US_NYSE_BOOK_20130403</code> for model training and <code>EQY_US_NYSE_BOOK_20130404</code> for model validation.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
object DecisionTreeDynamics <span class="directive">extends</span> App with ConfiguredSparkContext with FeaturesExtractor {
  <span class="directive">private</span> val log = LoggerFactory.getLogger(<span class="local-variable">this</span>.getClass)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">case</span> <span class="type">class</span> <span class="class">Config</span>(<span class="key">training</span>: <span class="predefined-type">String</span> = <span class="error">“</span><span class="error">”</span>,
                    <span class="key">validation</span>: <span class="predefined-type">String</span> = <span class="error">“</span><span class="error">”</span>,
                    <span class="key">filter</span>: <span class="predefined-type">Option</span>[<span class="predefined-type">String</span>] = None,
                    <span class="key">symbol</span>: <span class="predefined-type">Option</span>[<span class="predefined-type">String</span>] = None)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;val parser = <span class="keyword">new</span> OptionParser&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">&amp;quot;Order Book Dynamics&amp;quot;</span><span class="delimiter">&quot;</span></span>&gt;Config&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt; {</span></span><span class="error">
</span>    <span class="comment">// ….</span>
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;parser.parse(args, Config()) map { implicit config =&amp;gt;
    val trainingFiles = openBookFiles(<span class="error">“</span>Training<span class="error">”</span>, config.training, config.filter)
    val validationFiles = openBookFiles(<span class="error">“</span>Validation<span class="error">”</span>, config.validation, config.filter)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;val trainingOrderLog = orderLog(trainingFiles)
log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Training order log size: </span><span class="inline"><span class="inline-delimiter">${</span>trainingOrderLog.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)

<span class="comment">// Configure DecisionTree model</span>
val labelEncode = implicitly[LabelEncode[MeanPriceMove]]
val numClasses = labelEncode.numClasses
val categoricalFeaturesInfo = <span class="predefined-type">Map</span>.empty[Int, Int]
val impurity = <span class="string"><span class="delimiter">&quot;</span><span class="content">gini</span><span class="delimiter">&quot;</span></span>
val maxDepth = <span class="integer">5</span>
val maxBins = <span class="integer">100</span>

val trainingData = trainingOrderLog.extractLabeledData(featuresExtractor(<span class="key">_</span>: <span class="predefined-type">String</span>))
val trainedModels = (trainingData map { <span class="keyword">case</span> LabeledOrderLog(symbol, labeledPoints) =&amp;gt;
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Train Decision Tree model. Training data size: </span><span class="inline"><span class="inline-delimiter">${</span>labeledPoints.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
  val model = DecisionTree.trainClassifier(labeledPoints, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)
  val labelCounts = labeledPoints.map(_.label).countByValue().map {
    <span class="keyword">case</span> (key, count) =&amp;gt; (labelEncode.decode(key.toInt), count)
  }
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Label counts: [</span><span class="inline"><span class="inline-delimiter">${</span>labelCounts.mkString(<span class="string"><span class="delimiter">&quot;</span><span class="content">, </span><span class="delimiter">&quot;</span></span>)<span class="inline-delimiter">}</span></span><span class="content">]</span><span class="delimiter">&quot;</span></span>)
  symbol -&amp;gt; model
}).toMap

val validationOrderLog = orderLog(validationFiles)
log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Validation order log size: </span><span class="inline"><span class="inline-delimiter">${</span>validationOrderLog.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
val validationData = validationOrderLog.extractLabeledData(featuresExtractor(<span class="key">_</span>: <span class="predefined-type">String</span>))

<span class="comment">// Evaluate model on validation data and compute training error</span>
validationData.map { <span class="keyword">case</span> LabeledOrderLog(symbol, labeledPoints) =&amp;gt;

  val model = trainedModels(symbol)

  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Evaluate model on validation data. Validation data size: </span><span class="inline"><span class="inline-delimiter">${</span>labeledPoints.count()<span class="inline-delimiter">}</span></span><span class="delimiter">&quot;</span></span>)
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Learned classification tree model: </span><span class="inline"><span class="inline-delimiter">$</span>model</span><span class="delimiter">&quot;</span></span>)

  val labelAndPrediction = labeledPoints.map { point =&amp;gt;
    val prediction = model.predict(point.features)
    (point.label, prediction)
  }
  val trainingError = labelAndPrediction.filter(r =&amp;gt; r._1 != r._2).count().toDouble / labeledPoints.count
  log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="inline"><span class="inline-delimiter">$</span>symbol</span><span class="content">: Training Error = </span><span class="delimiter">&quot;</span></span> + trainingError)
}   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<h4 id="training-error">Training Error</h4>

<p>Output of running Decision Tree classification for single symbol <code>ORCL</code>:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
ORCL: Train Decision Tree model. Training data size: 64064
ORCL: Trained model in 3740 millis
ORCL: Label counts: [Stationary -&amp;gt; 42137, Down -&amp;gt; 10714, Up -&amp;gt; 11213]
ORCL: Evaluate model on validation data. Validation data size: 54749
ORCL: Training Error = 0.28603262160039455
</pre></div>
</div>
 </figure></notextile></div></p>

<p>As you can see this pretty simple model was able to successfully classify ~70% of the data.</p>

<p><strong>Remark:</strong> Despite the fact, that this model shows very good success rate, it doesn’t mean that it
can be successfully used to build profitable automated trading strategy. First of all I don’t check
if it’s 95% success predicting stationary and 95% error rate predicting any price movement with
average 70% success rate. It doesn’t measure “strength” of price movement, it has to be sufficient to cover
transaction costs. And many other details that matters for building real trading system.</p>

<p>For sure it’s huge room for improvement and result validation. Unfortunately it’s hard do get enough data,
2 trading days is to small data set to draw conclusions and start building system to earn all the money in the world.
However I think it’ a good starting point.</p>

<h3 id="results">Results</h3>

<p>I was able to relatively easy reproduce fairly complicated research project at much lager scale than in original paper.</p>

<p>Latest Big Data technologies allows to build models using all available data, and stop doing samplings.
Using all of the data helps to build best possible models and capture all details from full data set.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/orderbook-dynamics">Github</a></p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Spark Tests in Standalone Cluster]]></title>
    <link href="http://eugenezhulenev.com/blog/2014/10/18/run-tests-in-standalone-spark-cluster/"/>
    <updated>2014-10-18T21:01:15-04:00</updated>
    <id>http://eugenezhulenev.com/blog/2014/10/18/run-tests-in-standalone-spark-cluster</id>
    <content type="html"><![CDATA[<p>Unit testing Spark Applications with standalone Apache Spark Cluster.</p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/spark-testing">Github</a></p>
</blockquote>

<h3 id="running-spark-applications">Running Spark Applications</h3>

<p>To be able to run Spark jobs, Spark cluster needs to have all classes used by your application in it’s classpath.
You can put manually all jar files required by your application to Spark nodes, but it’s not cool.
Another solution is to manually set jar files that required to distribute to worker nodes
when you create SparkConf. One way to do it, is to package your application as a “fat-jar”,
so you need to distribute only single jar.
Industry standard for packaging Spark application is <a href="https://github.com/sbt/sbt-assembly">sbt-assembly</a> plugin,
and it’s used by Spark itself.</p>

<h3 id="unit-testing-spark-applications">Unit Testing Spark Applications</h3>

<p>If you need to test your Spark application, easiest way is to create local Spark Context for each test, or maybe shared between all tests.
When Spark is running in local mode, it’s running in the same JVM as your tests with same jar files in classpath.</p>

<p>If your tests requires data that doesn’t fit into single node, for example in integration or acceptance tests,
obvious solution is to run them in standalone Spark cluster
with sufficient number of nodes. At this time everything becomes more difficult. Now you need to package you application with tests
in single jar file, and submit it to Spark cluster with each test.</p>

<!-- more -->

<h3 id="example-application">Example Application</h3>

<p>To show how to run and test Spark applications I prepared very <a href="https://github.com/ezhulenev/spark-testing">simple application</a>.
It uses <a href="https://github.com/ezhulenev/scala-openbook">Scala OpenBook</a>
library to parse <a href="http://www.nyxdata.com/Data-Products/NYSE-OpenBook-History">NYSE OpenBook</a> messages (orders log from New York Stock Exchange),
distribute them to cluster as RDD, and count Buy and Sell orders by ticker.
Only purpose of this application is to have dependency on a library that for sure is not available on Spark nodes.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="type">class</span> <span class="class">OrdersFunctions</span>(<span class="annotation">@transient</span> <span class="key">sc</span>: SparkContext, <span class="key">orders</span>: <span class="predefined-type">Iterator</span>[OpenBookMsg]) <span class="directive">extends</span> <span class="predefined-type">Serializable</span> {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> val ordersRDD = sc.parallelize(orders.toSeq)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">countBuyOrders</span>(): <span class="predefined-type">Map</span>[<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>] = countOrders(OrderFunctions.isBuySide)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">def</span> <span class="function">countSellOrders</span>(): <span class="predefined-type">Map</span>[<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>] = countOrders(OrderFunctions.isSellSide)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> <span class="keyword">def</span> <span class="function">countOrders</span>(<span class="key">filter</span>: OpenBookMsg =&amp;gt; <span class="predefined-type">Boolean</span>): <span class="predefined-type">Map</span>[<span class="predefined-type">String</span>, <span class="predefined-type">Long</span>] =
    ordersRDD.filter(filter).
      map(order =&amp;gt; (order.symbol, order)).
      countByKey().toMap&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;}
</pre></div>
</div>
 </figure></notextile></div></p>

<p> </p>

<h3 id="assembly-main-application">Assembly Main Application</h3>

<p>Add sbt-assembly plugin in project/plugin.sbt</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
addSbtPlugin(<span class="error">“</span>com.eed3si9n<span class="error">”</span> % <span class="error">“</span>sbt-assembly<span class="error">”</span> % <span class="error">“</span><span class="float">0.11</span><span class="float">.2</span><span class="error">”</span>)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Add assembly settings to build.sbt</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Merge strategy shared between app &amp;amp; test&lt;/p&gt;</span>

&lt;p&gt;val <span class="key">sharedMergeStrategy</span>: (<span class="predefined-type">String</span> =&amp;gt; MergeStrategy) =&amp;gt; <span class="predefined-type">String</span> =&amp;gt; MergeStrategy =
  old =&amp;gt; {
    <span class="keyword">case</span> x <span class="keyword">if</span> x.startsWith(<span class="error">“</span>META-INF/ECLIPSEF.RSA<span class="error">”</span>) =&amp;gt; MergeStrategy.last
    <span class="keyword">case</span> x <span class="keyword">if</span> x.startsWith(<span class="error">“</span>META-INF/mailcap<span class="error">”</span>) =&amp;gt; MergeStrategy.last
    <span class="keyword">case</span> x <span class="keyword">if</span> x.endsWith(<span class="error">“</span>plugin.properties<span class="error">”</span>) =&amp;gt; MergeStrategy.last
    <span class="keyword">case</span> x =&amp;gt; old(x)
  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Load Assembly Settings&lt;/p&gt;</span>

&lt;p&gt;assemblySettings&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="comment">// Assembly App&lt;/p&gt;</span>

&lt;p&gt;mainClass <span class="keyword">in</span> assembly := Some(<span class="error">“</span>com.github.ezhulenev.spark.RunSparkApp<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;jarName <span class="keyword">in</span> assembly := <span class="error">“</span>spark-testing-example-app.jar<span class="error">”</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;mergeStrategy <span class="keyword">in</span> assembly <span class="error">«</span>= (mergeStrategy <span class="keyword">in</span> assembly)(sharedMergeStrategy)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>Inside your application you need to create SparkConf and add current jar to it.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
  <span class="keyword">new</span> SparkConf().
      setMaster(<span class="error">“</span><span class="key">spark</span>:<span class="comment">//spark-host:7777”).</span>
      setJars(SparkContext.jarOfClass(<span class="local-variable">this</span>.getClass).toSeq).
      setAppName(<span class="error">“</span>SparkTestingExample<span class="error">”</span>)
</pre></div>
</div>
 </figure></notextile></div></p>

<p>After that you can use assembly command, and run assembled application in your Spark Cluster</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
&amp;gt; sbt assembly
&amp;gt; java -Dspark.master=spark://spark-host:7777 target/scala_2.10/spark-testing-example-app.jar
</pre></div>
</div>
 </figure></notextile></div></p>

<p> </p>

<h3 id="assembly-tests">Assembly Tests</h3>

<p>First step to run tests in standalone Spark Cluster is to package all main and test classes into single jar, that will be
transfered to each worker node before running tests. It’s very similar to assemblying main app.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
<span class="comment">// Assembly Tests&lt;/p&gt;</span>

&lt;p&gt;Project.inConfig(Test)(assemblySettings)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;jarName <span class="keyword">in</span> (Test, assembly) := <span class="error">“</span>spark-testing-example-tests.jar<span class="error">”</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;mergeStrategy <span class="keyword">in</span> (Test, assembly) <span class="error">«</span>= (mergeStrategy <span class="keyword">in</span> assembly)(sharedMergeStrategy)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;test <span class="keyword">in</span> (Test, assembly) := {} <span class="comment">// disable tests in assembly</span>
</pre></div>
</div>
 </figure></notextile></div></p>

<p>I wrote simple sbt plugin that has <code>test-assembly</code> task. First this task assemblies jar
file with test classes and all dependencies, then set it’s location
to environment variable, and then starts tests.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
object TestWithSparkPlugin <span class="directive">extends</span> sbt.Plugin {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="keyword">import</span> <span class="include">TestWithSparkKeys._</span>
  <span class="keyword">import</span> <span class="include">AssemblyKeys._</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object TestWithSparkKeys {
    lazy val testAssembled        = TaskKey&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">&amp;quot;test-assembled&amp;quot;,</span><span class="delimiter">&quot;</span></span> title=<span class="string"><span class="delimiter">&quot;</span><span class="content">Run tests with standalone Spark cluster</span><span class="delimiter">&quot;</span></span>&gt;Unit&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;</span></span><span class="error">
</span>    lazy val assembledTestsProp   = SettingKey&lt;a href=<span class="string"><span class="delimiter">&quot;</span><span class="content">&amp;quot;assembled-tests-prop&amp;quot;,</span><span class="delimiter">&quot;</span></span> title=<span class="string"><span class="delimiter">&quot;</span><span class="content">Environment variable name used to pass assembled jar name to test</span><span class="delimiter">&quot;</span></span>&gt;<span class="predefined-type">String</span>&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">a&gt;</span></span><span class="error">
</span>  }&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;lazy val <span class="key">baseTestWithSparkSettings</span>: Seq[sbt.Def.Setting[_]] = Seq(
    testAssembled        := TestWithSpark.testWithSparkTask.value,
    assembledTestsProp   := <span class="error">“</span>ASSEMBLED_TESTS<span class="error">”</span>
  )&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;lazy val <span class="key">testWithSparkSettings</span>: Seq[sbt.Def.Setting[_]] = baseTestWithSparkSettings&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;object TestWithSpark {&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;<span class="keyword">def</span> <span class="key">assemblyTestsJarTask</span>: Initialize[Task[<span class="predefined-type">File</span>]] = Def.task {
  val assembled = (assembly <span class="keyword">in</span> Test).value
  sys.props(assembledTestsProp.value) = assembled.getAbsolutePath
  assembled
}

<span class="directive">private</span> <span class="keyword">def</span> runTests = Def.task {
  (test <span class="keyword">in</span> Test).value
}

<span class="keyword">def</span> <span class="key">testWithSparkTask</span>: Initialize[Task[Unit]] = Def.sequentialTask {
  assemblyTestsJarTask.value
  runTests.value
}   } } </pre></div>
</div>
 </figure></notextile></div>
</code></pre>

<p>All Apache Spark tests should inherit <code>ConfiguredSparkFlatSpec</code> with configured Spark Context. If assembled tests jar file
is available, it’s distributed to Spark worker nodes. If not, only local mode is supported.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
trait ConfiguredSparkFlatSpec <span class="directive">extends</span> FlatSpec with BeforeAndAfterAll {
  <span class="directive">private</span> val log = LoggerFactory.getLogger(classOf[ConfiguredSparkFlatSpec])&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> val config = ConfigFactory.load()&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;<span class="directive">private</span> lazy val sparkConf = {
    val master = config.getString(<span class="error">“</span>spark.master<span class="error">”</span>)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;pre&gt;&lt;code&gt;log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Create spark context. Master: </span><span class="inline"><span class="inline-delimiter">$</span>master</span><span class="delimiter">&quot;</span></span>)
val assembledTests = sys.props.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">ASSEMBLED_TESTS</span><span class="delimiter">&quot;</span></span>)

val baseConf = <span class="keyword">new</span> SparkConf().
  setMaster(master).
  setAppName(<span class="string"><span class="delimiter">&quot;</span><span class="content">SparkTestingExample</span><span class="delimiter">&quot;</span></span>)

assembledTests match {
  <span class="keyword">case</span> None =&amp;gt;
    log.warn(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Assembled tests jar not found. Standalone Spark mode is not supported</span><span class="delimiter">&quot;</span></span>)
    baseConf
  <span class="keyword">case</span> Some(path) =&amp;gt;
    log.info(s<span class="string"><span class="delimiter">&quot;</span><span class="content">Add assembled tests to Spark Context from: </span><span class="inline"><span class="inline-delimiter">$</span>path</span><span class="delimiter">&quot;</span></span>)
    baseConf.setJars(path :: Nil)
}   }
&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">code&gt;&lt;</span><span class="delimiter">/</span></span>pre&gt;

&lt;p&gt;lazy val sc = <span class="keyword">new</span> SparkContext(sparkConf)&lt;<span class="regexp"><span class="delimiter">/</span><span class="content">p&gt;</span></span><span class="error">
</span>
&lt;p&gt;override <span class="directive">protected</span> <span class="keyword">def</span> <span class="function">afterAll</span>(): Unit = {
    <span class="local-variable">super</span>.afterAll()
    sc.stop()
  }
}
</pre></div>
</div>
 </figure></notextile></div></p>

<p> </p>

<h3 id="running-tests">Running Tests</h3>

<p>By default <code>spark.master</code> property is set to local[2]. So you can run tests in local mode. If you want run tests
in standalone Apache Spark, you need to override <code>spark.master</code> with your master node.</p>

<p>If you’ll try to run <code>test</code> command with standalone cluster it will fail with ClassNotFoundException</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
&amp;gt; sbt -Dspark.master=spark://spark-host:7777 test
&amp;gt;
&amp;gt; Create spark context. Master: spark://Eugenes-MacBook-Pro.local:7077
&amp;gt; Assembled tests jar not found. Standalone Spark mode is not supported
&amp;gt;
&amp;gt; [error] Failed tests:
&amp;gt; org.apache.spark.SparkException: Job aborted due to stage failure:
&amp;gt; Task 2 in stage 1.0 failed 4 times, most recent failure:
&amp;gt; Lost task 2.3 in stage 1.0 (TID 30, 192.168.0.11):
&amp;gt; java.lang.ClassNotFoundException: com.scalafi.openbook.OpenBookMsg
</pre></div>
</div>
 </figure></notextile></div></p>

<p>However <code>test-assembled</code> will be successfull</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
&amp;gt; sbt -Dspark.master=spark://spark-host:7777 test-assembled
&amp;gt;
&amp;gt; Create spark context. Master: spark://Eugenes-MacBook-Pro.local:7077
&amp;gt; Add assembled tests to Spark Context from: /Users/ezhulenev/spark-testing/target/scala-2.10/spark-testing-example-tests.jar
&amp;gt;
&amp;gt; [info] Run completed in 7 seconds, 587 milliseconds.
&amp;gt; [info] Total number of tests run: 2
&amp;gt; [info] Suites: completed 1, aborted 0
&amp;gt; [info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0
&amp;gt; [info] All tests passed.
&amp;gt; [success] Total time: 37 s
</pre></div>
</div>
 </figure></notextile></div></p>

<p> </p>

<blockquote>
  <p>The code for this application app can be found on <a href="https://github.com/ezhulenev/spark-testing">Github</a></p>
</blockquote>
]]></content>
  </entry>
  
</feed>
