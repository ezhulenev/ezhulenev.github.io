<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ec2 | Eugene Zhulenev]]></title>
  <link href="http://eugenezhulenev.com/blog/categories/ec2/atom.xml" rel="self"/>
  <link href="http://eugenezhulenev.com/"/>
  <updated>2016-12-25T13:10:28-05:00</updated>
  <id>http://eugenezhulenev.com/</id>
  <author>
    <name><![CDATA[Eugene Zhulenev]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Large Scale Deep Learning With TensorFlow on EC2 Spot Instances]]></title>
    <link href="http://eugenezhulenev.com/blog/2016/02/01/deep-learning-with-tensorflow-on-ec2-spot-instances/"/>
    <updated>2016-02-01T11:14:16-05:00</updated>
    <id>http://eugenezhulenev.com/blog/2016/02/01/deep-learning-with-tensorflow-on-ec2-spot-instances</id>
    <content type="html"><![CDATA[<p>In this post I’m demonstrating how to combine together <strong>TensorFlow</strong>, <strong>Docker</strong>, <strong>EC2 Container Service</strong> and <strong>EC2 Spot Instances</strong> 
to solve massive cluster computing problems the most cost-effective way.</p>

<blockquote>
  <p>Source code is on on Github: <a href="https://github.com/ezhulenev/distributo">https://github.com/ezhulenev/distributo</a></p>
</blockquote>

<p>Neural Networks and Deep Learning in particular gained a lot of attention over the last year, and it’s only the beginning. Google released
to open source their numerical computing framework <a href="https://www.tensorflow.org/">TensorFlow</a>, which can be used for training and running
deep neural networks for wide variety of machine learning problems, especially image recognition.</p>

<blockquote>
  <p>TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google’s Machine Intelligence research 
organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be 
applicable in a wide variety of other domains as well.</p>
</blockquote>

<p>Although TensorFlow version used at Google supports distributed training, open sourced version can run only on one node. However some of machine learning
problems are still embarrassingly parallel, and can be easily parallelized regardless of single-node nature of the core library itself.</p>

<ol>
  <li><strong>Hyperparameter optimization</strong> or <strong>model selection</strong> is the problem of choosing a set of hyperparameters for a learning algorithm, 
usually with the goal of optimizing a measure of the algorithm’s performance on an independent data set. Naturally parallelized by training
models for each set ot parameters in parallel and choosing the best model (parameters) later.</li>
  <li><strong>Inference</strong> (applying trained model to new data) can be parallelized by splitting input dataset into smaller 
batches and running trained model on each of them in parallel</li>
</ol>

<!-- more -->

<h2 id="hyperparameter-optimization">Hyperparameter Optimization</h2>

<p>TensorFlow provides primitives for expressing and solving various numerical computations using data flow graphs, and it was primarily designed to solve
neural networks research problems.</p>

<p>Choosing a right design and parameters for your neural network is separate optimization problem:  </p>

<ul>
  <li>how many layers to use?</li>
  <li>how many neurons in each layer?</li>
  <li>what learning rate to use?</li>
</ul>

<p>These choices form a set of model hyperparameters, which can be used for training multiple models in parallel.</p>

<p><img class="center" src="/images/deep-learning/hyperparameter-optimization.png" title="Hyperparameter Optimization" ></p>

<h2 id="inference">Inference</h2>

<p>When you already have trained model, and you want to score/classify huge dataset, you can use similar approach: split all your input
data into smaller batches, and run them in parallel. Instead of different hyperparameters, scheduler will control batch offsets 
defining what part of the dataset should be loaded for inference.</p>

<h1 id="tensorflow-for-image-recognition">TensorFlow for Image Recognition</h1>

<h2 id="packaging-into-docker-image">Packaging into Docker Image</h2>

<p>TensorFlow has awesome <a href="https://www.tensorflow.org/versions/0.6.0/tutorials/image_recognition/index.html">Image Recognition Tutorial</a>, which uses already
pre-trained ImageNet model for image recognition/classification. You provide an image, and a model gives you what it 
can see on this image: leopard, container ship, place, etc.. Works like magic.</p>

<p>I’ve prepared <a href="https://github.com/ezhulenev/distributo/tree/master/example/docker-tensorflow">Docker image</a> based on official TensorFlow image 
and slightly modified image classification example. It takes a range of images that needs to be classified, and S3 
path where to put results:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
# You have to provider your AWS credentials to upload files on S3
# and S3 bucket name
docker run -it -e ‘AWS_ACCESS_KEY_ID=…’ -e ‘AWS_SECRET_ACCESS_KEY=…’ \
        ezhulenev/distributo-tensorflow-example \
        0:100 s3://distributo-example/imagenet/inferred-0-100.txt
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This command will classify first 100 images from <a href="http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz">http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz</a>:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
n00005787&lt;em&gt;175   http://farm3.static.flickr.com/2179/2090355369&lt;/em&gt;4c8a60f899.jpg
n00005787&lt;em&gt;185   http://farm4.static.flickr.com/3205/2815917575_c2ea596ed2.jpg
n00005787&lt;/em&gt;186   http://farm3.static.flickr.com/2084/2517885309&lt;em&gt;6680a79ab1.jpg
n00005787&lt;/em&gt;190   http://farm1.static.flickr.com/81/245539781&lt;em&gt;42028c8c67.jpg
n00005787&lt;/em&gt;198   http://farm2.static.flickr.com/1437/680424989&lt;em&gt;da45c42286.jpg
n00005787&lt;/em&gt;219   http://farm1.static.flickr.com/176/441681804_fec8ae4c58.jpg
</pre></div>
</div>
 </figure></notextile></div></p>

<p>And upload inference results to S3:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
n00005787&lt;em&gt;175,http://farm3.static.flickr.com/2179/2090355369&lt;/em&gt;4c8a60f899.jpg,[
  (‘brain coral’, 0.354022), 
  (‘hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa’, 0.18448937), 
  (‘coral reef’, 0.15611894), 
  (‘gyromitra’, 0.035839655), 
  (‘coral fungus’, 0.03291795)
]
n00005787_185,http://farm4.static.flickr.com/3205/2815917575_c2ea596ed2.jpg,[
  (‘sea slug, nudibranch’, 0.4032793), 
  (‘sea cucumber, holothurian’, 0.17277676), 
  (‘hermit crab’, 0.043269496),
  (‘conch’, 0.036443222), 
  (‘jellyfish’, 0.023511186)
]
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="moving-into-the-aws-cloud">Moving into the AWS Cloud</h2>

<p>Amazon has <a href="https://aws.amazon.com/ecs/">EC2 Container Service (ECS)</a>, which is container management service that supports Docker containers,
and allows to easily launch any task packaged into container on EC2 instances using simple API. You don’t have to worry about managing
your cluster or installing any additional software. It just works out of the box with Amazon provided AMIs.</p>

<p>ECS clusters are running on regular EC2 instances, and it’s up to you what instances to use. One option, that especially makes sense
for large offline model training/inference it to use <a href="https://aws.amazon.com/ec2/spot/">EC2 Spot Instances</a> which allow you to bid on spare EC2 
computing capacity. Spot instances are usually available at a big discount compared to On-Demand pricing, this allows to 
significantly reduce the cost of running computation, and scale only when price allows to do so.</p>

<h1 id="distributo">Distributo</h1>

<p>Distributo is a small library that makes it easier to automate EC2 resource allocation on spot market, 
and provides custom ECS scheduler that takes care of efficient execution of your tasks on available computing resources.</p>

<p>Source code is on Github: <a href="https://github.com/ezhulenev/distributo">https://github.com/ezhulenev/distributo</a>. </p>

<p>It requires <a href="http://leiningen.org/">Leiningen</a> to compile and to run example application.</p>

<h3 id="resource-allocator">Resource Allocator</h3>

<p>Resource allocator is responsible for allocating compute resources in EC2 based on outstanding 
jobs resource requirements. Right now it’s lame implementation that only supports fixed
size ECS cluster built from same type spot instances. You need to define upfront how many instances do you need.</p>

<h3 id="scheduler">Scheduler</h3>

<p>Scheduler decides on what available container instance to start pending jobs. It’s using bin-packing 
with fitness calculators (concept borrowed from <a href="https://github.com/Netflix/Fenzo/wiki/Fitness-Calculators">Netflix/Fenzo</a>) to 
choose best instance to start new task. It’s the main difference from default ECS scheduler that
places tasks on random instances.</p>

<h2 id="run-tensorflow-image-recognition-with-distributo">Run TensorFlow Image Recognition with Distributo</h2>

<p>Distributo uses AWS JAVA SDK to access your AWS credentials. If you don’t have them already configured you
can do it with AWS CLI</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
aws configure
</pre></div>
</div>
 </figure></notextile></div></p>

<p>After that you can start you cluster and run TensorFlow inference with this command:    </p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
lein run –inference \
  –num-instances 1 \
  –batch-size 100 \
  –num-batches 10 \
  –output s3://distributo-example/imagenet/
</pre></div>
</div>
 </figure></notextile></div></p>

<p>This command will run 10 TensorFlow containers with batches from <code>[0:100]</code> up to <code>[900:1000]</code> on single instance and 
put inference results into S3 bucket. By default it’s buying <code>m4.large</code> instances for <code>$0.03</code> which can run only 2 containers 
in parallel, in this example 10 jobs will be competing for 1 instance.</p>

<p>Distributo doesn’t free resources after it’s done with inference. If you are done, don’t forget to clean resources:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="CodeRay">
  <div class="code"><pre>
lein run –free-resources
</pre></div>
</div>
 </figure></notextile></div></p>

<h2 id="future-work">Future Work</h2>

<p>Resource allocator and scheduler could be much more clever about their choices of regions, availability zones 
and instance types to be able to build most price-effective cluster out of resources currently 
available on spot market.</p>

<p><a href="https://github.com/twosigma/Cook">TwoSigma/Cook</a> - has lot’s of great ideas about fair resource allocation and cluster 
sharing for large scale batch computations, which might be very interesting to implement</p>

<h2 id="alternative-approaches">Alternative Approaches</h2>

<h3 id="spark-as-distributed-compute-engine">Spark as Distributed Compute Engine</h3>

<p>Apache Spark has Python integration and it’s possible to achieve very similar parallelization 
with it: <a href="https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html">https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html</a>.</p>

<p>However it’s completely different from approach that I described, because it doesn’t allow to use Docker (easily) and requires non trivial cluster setup. Although it might be more powerful
because it’s much easier to build more complicated pipelines.</p>

<h3 id="aws-auto-scaling">AWS Auto Scaling</h3>

<p>ECS provides auto scaling out of the box, also it has it’s own task scheduler. However task scheduler use random 
containers to place new tasks, which can lead to unefficient resource utilization. And with custom resource allocator it’s 
possible to build more sophisticated strategies for buying cheapest computing resources on fluctuating market.</p>

<h3 id="mesos">Mesos</h3>

<p>Mesos also provides great API for running tasks packaged as Docker images in the cluster.</p>

<p>However managing Mesos deployment is not trivial task, and you might not want to do have this headache. Also it’s much more difficult to provide
truly scalable platform, you’ll have to provision your cluster for peak load, which can be expensive and not cost-effective.</p>
]]></content>
  </entry>
  
</feed>
